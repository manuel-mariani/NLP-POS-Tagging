{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3B9biLeVvc1"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency management\n",
    "!pip install tensorflow==2.7.0 numpy==1.21.4 pandas==1.3.4 requests==2.26.0 gensim==4.1.2 wandb==0.12.7 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.autograph.set_verbosity(0)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Model definition\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Data packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Cloning\n",
    "from copy import deepcopy\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook visualization\n",
    "from IPython.core.display import display\n",
    "\n",
    "# Typing\n",
    "from typing import Set, List, Dict, Tuple\n",
    "\n",
    "# For GloVe wrapper\n",
    "from gensim import downloader as gensloader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Plotting\n",
    "import plotly.express as px\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Metrics and utility\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seeds\n",
    "SEED = 1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## 1 - Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### 1.1 - Data loading\n",
    "First, we load the dataset and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './dependency_treebank'  # Change if dataset already present locally\n",
    "DATASET_URL = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not Path(ds_path).exists():\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']:\n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI9ECEh3Vvc6"
   },
   "source": [
    "### 1.2 - GloVe loading\n",
    "Then, we load the GloVe embeddings (GloVe-50, to be precise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 50\n",
    "GLOVE_TYPE = f'glove-wiki-gigaword-{EMB_DIM}'\n",
    "GLOVE_FILE = f'./glove/glove-wiki-gigaword-{EMB_DIM}.kv'\n",
    "\n",
    "\n",
    "def load_glove(gl_file: str, gl_type: str) -> KeyedVectors:\n",
    "    # Load local version\n",
    "    path = Path(gl_file)\n",
    "    if path.exists():\n",
    "        return KeyedVectors.load(gl_file)\n",
    "\n",
    "    # Otherwise download and store glove\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    glove = gensloader.load(gl_type)\n",
    "    glove.save(gl_file)\n",
    "    return glove\n",
    "\n",
    "\n",
    "glove = load_glove(GLOVE_FILE, GLOVE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GloVe loading\n",
    "print(f'cat = {glove[\"cat\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### 1.3 - Pre-processing\n",
    "Our dataset is already relatively clean; however, one point that might be worth considering is how to handle lowercase conversions. Some tokens in our dataset will be intrinsically capitalized (e.g. proper nouns, the personal pronoun \"I\"), whereas some other will be capitalized only because they follow a period in the sentence they occur in.\n",
    "\n",
    "One might think of converting a token to lowercase based on its tag (e.g. if a token is a proper noun, keep it capitalized); however, to be fair, this could only be done on the training set, since in a real scenario test-set tags would be unknown.\n",
    "\n",
    "Anyway, all these considerations hold only if GloVe contains embeddings of capitalized words; if that's not the case, every word we keep as capitalized will be classified as OOV when matched with GloVe, even when their lowercase embedding actually exists.\n",
    "\n",
    "As it turns out, Glove does not encode capitalized words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_capitalized = len(\n",
    "    list(filter(lambda w: w[0].isupper(), glove.key_to_index.keys())))\n",
    "\n",
    "print(f'GloVe-50 encodes {num_capitalized} capitalized words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kqy2eiLVvdC"
   },
   "source": [
    "Therefore, we will be forced to convert all tokens to lowercase.\n",
    "\n",
    "We are also interested to see which \"special\" tokens are encoded in GloVe, i.e. punctuation, quotation marks, and tokens such as \"-LRB-\" and \"-RRB-\", which in our dataset replace \"(\" and \")\", respectively.\n",
    "\n",
    "As it turns out, GloVe contains every special symbol we care about, except for tokens reserved to brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    *',.:;\"`$#Â£!%/?^-()[]{}_', \"''\", \"``\", \"--\", \"-LRB-\", \"-RRB-\", \"-LSB-\",\n",
    "    \"-RSB-\", \"-LCB-\", \"-RCB-\"\n",
    "]\n",
    "for st in special_tokens:\n",
    "    if st not in glove:\n",
    "        print(f\"GloVe does not contain token {st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2B391arVvdD"
   },
   "source": [
    "Based on the previous considerations, we convert all tokens to lowercase and replace \"-LRB\"-like symbols with the corresponding bracket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the brackets\n",
    "for token, bracket in [('-LRB-', '('), ('-RRB-', ')'), ('-LSB-', '['),\n",
    "                       ('-RSB-', ']'), ('-LCB-', '{'), ('-RCB-', '}')]:\n",
    "    dataset.loc[dataset.token == token, 'token'] = bracket\n",
    "\n",
    "# Convert dataset tokens to lowercase\n",
    "dataset.loc[:, 'token'] = dataset['token'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLuhT_wCVvdE"
   },
   "source": [
    "### 1.4 - Splitting\n",
    "After pre-processing the data, we can split the dataset into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DOC_UB = 99\n",
    "TEST_DOC_LB = 150\n",
    "\n",
    "ds_train = dataset[dataset['document'].le(TRAIN_DOC_UB)]\n",
    "ds_val = dataset[dataset['document'].between(\n",
    "    TRAIN_DOC_UB, TEST_DOC_LB, inclusive='neither')].reset_index()\n",
    "ds_test = dataset[dataset['document'].ge(TEST_DOC_LB)].reset_index()\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} tokens\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(ds_train)}\n",
    "    VALIDATION: {print_split(ds_val)}\n",
    "    TEST: {print_split(ds_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70PQNGtHVvdE"
   },
   "source": [
    "### 1.5 - OOV Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dii_iOaHI4t1"
   },
   "source": [
    "#### 1.5.1 - OOV Analysis\n",
    "First of all, let us take a look at how many Out-Of-Vocabulary tokens (w.r.t. GloVe) our dataset contains. In order to simulate a real-world scenario, in which test samples are not readily available at training time, we are going to check (and then handle) OOVs *incrementally*; that is, we will consider:\n",
    "* **OOV1:** training-set tokens which are not found in V1 = GloVe.\n",
    "* **OOV2:** validation-set tokens which are not found in V2 = `union(`V1, OOV1`)`.\n",
    "* **OOV3:** test-set tokens which are not found in V3 = `union(`V2, OOV2`)`.\n",
    "\n",
    "Our final vocabulary, which will provide encodings to our model(s), is V4 = `union(`V3, OOV3`)` = `union(`GloVe, OOV1, OOV2, OOV3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_keys = set(glove.key_to_index.keys())\n",
    "oov1 = set(ds_train['token']) - glove_keys\n",
    "v2 = glove_keys.union(oov1)\n",
    "oov2 = set(ds_val['token']) - v2\n",
    "v3 = v2.union(oov2)\n",
    "oov3 = set(ds_test['token']) - v3\n",
    "\n",
    "print(f'OOV1:  {len(oov1)} tokens')\n",
    "print(f'OOV2:  {len(oov2)} tokens')\n",
    "print(f'OOV3:  {len(oov3)} tokens')\n",
    "l = len(oov1) + len(oov2) + len(oov3)\n",
    "print(\n",
    "    f'Total: {l} tokens ({l / len(set(dataset[\"token\"])) * 100:.2f}% of dataset)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJkqb0b9VvdH"
   },
   "source": [
    "#### 1.5.2 - Adding OOVs to GloVe\n",
    "We can now add OOV tokens to the GloVe vocabulary. Many strategies can be adopted to encode OOVs as vectors:\n",
    "1. Static embeddings with the same vector for all OOV tokens (e.g. zeros).\n",
    "2. Random embeddings. \n",
    "3. Computing an embedding as some statistic involving neighboring tokens (e.g. their mean).\n",
    "\n",
    "Two observations can guide us in the choice of an embedding strategy:\n",
    "* OOV tokens are not negligible (about 6% of the *total* dataset).\n",
    "* Our GloVe embeddings will not undergo further training, therefore fixed or random embedding values will not be refined during the training process.\n",
    "\n",
    "For the two reasons above, given an OOV token, we will compute its embedding as the mean of its left and right neighbors across all its occurrences throughout the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighbor_mean(oov_token: str, df: pd.DataFrame,\n",
    "                          embeddings: KeyedVectors) -> np.ndarray:\n",
    "    # Find indexes where the oov token appears, and shift them by -1 +1\n",
    "    indexes = df.index[df['token'] == oov_token].values\n",
    "    indexes = np.concatenate((indexes - 1, indexes + 1))\n",
    "\n",
    "    # For each oov word index, look at the left and right until a word with embedding has been found\n",
    "    neighbor_embeddings = []\n",
    "    for idx in indexes:\n",
    "        for direction in (range(idx - 1, -1, -1), range(idx + 1, len(df))):\n",
    "            for i in direction:\n",
    "                tok = df['token'].iloc[i]\n",
    "                if tok not in embeddings:\n",
    "                    continue\n",
    "                vector = embeddings[tok]\n",
    "                neighbor_embeddings.append(vector)\n",
    "                break\n",
    "\n",
    "    return np.mean(neighbor_embeddings, axis=0)\n",
    "\n",
    "\n",
    "def add_oovs(oov_tokens: Set, df: pd.DataFrame,\n",
    "             embeddings: KeyedVectors) -> KeyedVectors:\n",
    "    # Clone the embedding (KeyedVectors does not have a clone method)\n",
    "    emb_filled = deepcopy(embeddings)\n",
    "\n",
    "    # Estimate the OOV embeddings\n",
    "    keys, values = [], []\n",
    "    for oov in oov_tokens:\n",
    "        vector = compute_neighbor_mean(oov, df, emb_filled)\n",
    "        keys.append(oov)\n",
    "        values.append(vector)\n",
    "    # Add the estimates to the embedding\n",
    "    emb_filled.add_vectors(keys, values)\n",
    "    return emb_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 = union(Glove, OOV1)\n",
    "# where neighbors of OOV1 are taken from the trainig set\n",
    "embeddings = add_oovs(oov1, ds_train, glove)\n",
    "\n",
    "# V3 = union(V2, OOV2)\n",
    "# where neighbors of OOV2 are taken from the validation set\n",
    "embeddings = add_oovs(oov2, ds_val, embeddings)\n",
    "\n",
    "# V4 = union(V3, OOV3)\n",
    "# where neighbors of OOV3 are taken from the test set\n",
    "embeddings = add_oovs(oov3, ds_test, embeddings)\n",
    "\n",
    "# Test number of embeddings\n",
    "print(f'Number of vectors in original GloVe:                  {len(glove)}')\n",
    "print(\n",
    "    f'Number of vectors after incremental addition of OOVs: {len(embeddings)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIasDlmeJqMa"
   },
   "source": [
    "### 1.6 - Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indexes used for word -> index -> embedding\n",
    "word_index = {k: v + 1\n",
    "              for k, v in embeddings.key_to_index.items()\n",
    "              }  # +1 because index 0...\n",
    "vocab_size = len(embeddings) + 1  # ...will be reserved to padding\n",
    "\n",
    "# Define the embedding matrix\n",
    "embedding_matrix = np.zeros(shape=(vocab_size, EMB_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_matrix[index] = embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "assert np.all(embedding_matrix[embeddings.key_to_index['cat'] +\n",
    "                               1] == embeddings['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCdBJQmXHLNZ"
   },
   "source": [
    "### 1.7 - Data Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkaqUqR2Bfza"
   },
   "source": [
    "The input data of our model could be either whole documents or single sentences contained in those documents; we will choose sentences as input data.\n",
    "\n",
    "Tokens in each sentence will be converted to integer sequences and later fed into a static `Embedding` layer storing the matrix of Glove encodings + OOVs, which will provide the input to our model.\n",
    "\n",
    "The corresponding tagsâi.e. the output of our modelâwill be instead one-hot encoded. The rationale behind this choice is that tags are purely categorical data, hence encoding them as integer sequences would inject a notion of ordering into the model, which however is not reflected in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "flatten_1d = lambda nested_list: [li[0] for li in nested_list]\n",
    "\n",
    "# Convert tokens into sequences (their vocabulary indexes)\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.word_index = word_index\n",
    "token_indexes = tokenizer.texts_to_sequences_generator(dataset['token'].array)\n",
    "token_indexes = flatten_1d(token_indexes)\n",
    "\n",
    "# Convert tags into sequences\n",
    "# (as an intermediate step before one-hot encoding them)\n",
    "tag_to_int = {k: v + 1 for v, k in enumerate(dataset['tag'].unique())}\n",
    "num_tags = len(tag_to_int) + 1\n",
    "tokenizer = Tokenizer(filters='', lower=False)\n",
    "tokenizer.word_index = tag_to_int\n",
    "tag_indexes = tokenizer.texts_to_sequences_generator(dataset['tag'].array)\n",
    "tag_indexes = flatten_1d(tag_indexes)\n",
    "\n",
    "# Augment dataset with new data\n",
    "dataset['token_index'] = token_indexes\n",
    "dataset['tag_index'] = tag_indexes\n",
    "\n",
    "# Group dataset by 'sentence', aggregating remaining data into lists\n",
    "ds_sentences = dataset.groupby(['document', 'sentence']).agg(list)\n",
    "ds_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of sentence length in the training + validation set (leaving the test set aside) to determine what is an appropriate padded-sequence size (for batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_len = ds_sentences.query(\n",
    "    f'document <= {TEST_DOC_LB-1}')['token'].transform(len)\n",
    "len_quantile = sentences_len.quantile(.99)\n",
    "print(\"99th percentile of sentence length in training + validation set:\",\n",
    "      len_quantile)\n",
    "\n",
    "fig = px.histogram(sentences_len, labels={'value': 'Sentence Length'})\n",
    "fig.add_vline(len_quantile,\n",
    "              annotation_text=\"99th percentile\",\n",
    "              line_color='green')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 99th percentile suggests to trim sentences that exceed 56 tokens and pad sentences with fewer tokens, in order to prevent the few outliers from causing sentence encodings to be wastefully long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = lambda x: pad_sequences(\n",
    "    x, maxlen=int(len_quantile), padding='pre', truncating='pre')\n",
    "\n",
    "\n",
    "def get_model_data(df: pd.DataFrame, lb=0, ub=None):\n",
    "    df = df.query(f'{lb} <= document <= {ub}') if ub else df.query(\n",
    "        f'{lb} <= document')\n",
    "    toks = pad(df['token_index'])\n",
    "    tags = pad(df['tag_index'])\n",
    "    # One-hot encode tags\n",
    "    tags = to_categorical(tags, num_classes=num_tags)\n",
    "    return toks, tags\n",
    "\n",
    "\n",
    "# Build the data that will be fed to the model\n",
    "x_train, y_train = get_model_data(ds_sentences, 0, TRAIN_DOC_UB)\n",
    "x_val, y_val = get_model_data(ds_sentences, TRAIN_DOC_UB + 1, TEST_DOC_LB - 1)\n",
    "x_test, y_test = get_model_data(ds_sentences, TEST_DOC_LB)\n",
    "\n",
    "# Check shapes\n",
    "print(f\"\"\"\n",
    "X shapes [sentences x tokens]\n",
    "    x_train.shape = {x_train.shape}\n",
    "    x_val.shape   = {x_val.shape}\n",
    "    x_test.shape  = {x_test.shape}\n",
    "\n",
    "Y shapes [sentences x tags x one-hot-size]\n",
    "    y_train.shape = {y_train.shape}\n",
    "    y_val.shape   = {y_val.shape}\n",
    "    y_test.shape  = {y_test.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 - Class Imbalance Analysis\n",
    "It is reasonable to expect tags to be non-uniformly distributed throughout our dataset, as some parts of speech are intrinsically more common than others in any natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tag distribution\n",
    "px.histogram(dataset, x='tag').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, as shown by the histogram above, the tag distribution in out dataset is clearly imbalanced. In order to counteract this phenomenon during training, we are going to assign a (positive) weight to each tag â where such weights get bigger as tags get less common â and then apply those weights to the loss function. In other words, our models are going to get \"more rewarded\" when they correctly predict an uncommon tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weigths(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute class weights for unbalanced label vector y.\n",
    "\n",
    "    Args:\n",
    "         y: array of shape (n_samples, sequence_length, one_hot_length)\n",
    "            storing, for each sample, a sequence of one-hot encoded elements.\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape (n_samples, sequence_length) storing, for each sample,\n",
    "        a sequence of weights which are inversely proportional to the number \n",
    "        of occurrences of the corresponding sequence element in y.\n",
    "    \"\"\"\n",
    "    y_int = np.argmax(y, axis=-1)\n",
    "    class_weights = compute_class_weight('balanced',\n",
    "                                         classes=np.unique(y_int.flatten()),\n",
    "                                         y=y_int.flatten())\n",
    "    class_weights[0] = 0  # Ignore padding\n",
    "    int_to_weight = {i: w for i, w in zip(np.unique(y_int), class_weights)}\n",
    "    int_to_weight_v = np.vectorize(int_to_weight.get)\n",
    "    return int_to_weight_v(y_int)\n",
    "\n",
    "\n",
    "train_sample_weights = get_sample_weigths(y_train)\n",
    "val_sample_weights = get_sample_weigths(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CyRBTMKLT1i"
   },
   "source": [
    "## 2 - Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters shared by all models\n",
    "model_hyperparams = dict(\n",
    "    epochs=500,\n",
    "    batch_size=256,\n",
    "    mem_units=32,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "# Embedding layer hyperparameters â same for all models\n",
    "embedding_hyperparams = dict(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=EMB_DIM,\n",
    "    input_length=x_train.shape[-1],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Architectures\n",
    "Four model architectures will be compared, each with different combinations of recurrent and dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = model_hyperparams['mem_units']  # Number of memory units\n",
    "\n",
    "# Baseline: Bidirectional LSTM + Dense layer\n",
    "bi_lstm = Sequential([\n",
    "    Embedding(**embedding_hyperparams),\n",
    "    Bidirectional(LSTM(mu, return_sequences=True)),\n",
    "    Dense(num_tags, activation='softmax'),\n",
    "], 'BiLSTM-Dense')\n",
    "\n",
    "# Bidirectional GRU + Dense layer\n",
    "bi_gru = Sequential([\n",
    "    Embedding(**embedding_hyperparams),\n",
    "    Bidirectional((GRU(mu, return_sequences=True))),\n",
    "    Dense(num_tags, activation='softmax'),\n",
    "], 'BiGRU-Dense')\n",
    "\n",
    "# Bidirectional LSTM + Bidirectional LSTM + Dense layer\n",
    "double_bi_lstm = Sequential([\n",
    "    Embedding(**embedding_hyperparams),\n",
    "    Bidirectional(LSTM(mu, return_sequences=True)),\n",
    "    Bidirectional(LSTM(mu, return_sequences=True)),\n",
    "    Dense(num_tags, activation='softmax'),\n",
    "], '2xBiLSTM-Dense')\n",
    "\n",
    "# Bidirectional LSTM + Dense layer + Dense layer\n",
    "double_dense = Sequential([\n",
    "    Embedding(**embedding_hyperparams),\n",
    "    Bidirectional(LSTM(mu, return_sequences=True)),\n",
    "    Dense(2 * num_tags, activation='relu'),\n",
    "    Dense(num_tags, activation='softmax'),\n",
    "], 'BiLSTM-2xDense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training\n",
    "All four models will be trained for the same number of epochs and with the same hyperparameters (when comparable). During training, both accuracy and weighted accuracy will be monitored. When training ends, the only stored weigths will be those corresponding to the epoch at which the model reached maximum weighted accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true to do an anonymous session\n",
    "# and reproduce the results without logging into wandb\n",
    "ANON_SESSION = False\n",
    "RUN_TRAINING = False\n",
    "\n",
    "group_name = time.strftime('%Y%m%d-%H%M')\n",
    "models_history = dict()\n",
    "\n",
    "# Train and validate all models, logging data in wandb\n",
    "if RUN_TRAINING:\n",
    "    trained_runs_ids = []\n",
    "\n",
    "    for model in [bi_lstm, bi_gru, double_bi_lstm, double_dense]:\n",
    "\n",
    "        # Initialize wandb with relevant info\n",
    "        config = model_hyperparams\n",
    "        config['model-name'] = model.name\n",
    "        wandb_params = dict(\n",
    "            project='NLP-POS-Tagging',\n",
    "            name=model.name,\n",
    "            reinit=True,\n",
    "            config=config,\n",
    "            group=group_name,\n",
    "        )\n",
    "\n",
    "        if ANON_SESSION:\n",
    "            run = wandb.init(anonymous='must', **wandb_params)\n",
    "        else:\n",
    "            run = wandb.init(entity='frantoman', **wandb_params)\n",
    "\n",
    "        # Define a keras callback to save the model when it reaches\n",
    "        # max weighted validation accuracy\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=os.path.join(wandb.run.dir, f'{model.name}.h5'),\n",
    "            monitor='val_acc',\n",
    "            mode='max',\n",
    "        )\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=config['learning_rate']),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'],\n",
    "            # weighted_metrics=['acc'],\n",
    "        )\n",
    "        print(f'========= TRAINING MODEL: {model.name} {run.id}=========')\n",
    "        with run:\n",
    "\n",
    "            # Fit on data and run validation\n",
    "            history = model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                epochs=config['epochs'],\n",
    "                batch_size=config['batch_size'],\n",
    "                # validation_data=(x_val, y_val, val_sample_weights),\n",
    "                validation_data=(x_val, y_val),\n",
    "                # sample_weight=train_sample_weights,\n",
    "                callbacks=[WandbCallback(save_model=False), model_checkpoint],\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            # Log useful stuff\n",
    "            h = history.history\n",
    "            info = dict(\n",
    "                best_val_weighted_acc=np.max(h['val_acc']),\n",
    "                best_weighted_epoch=np.argmax(h['val_acc']) + 1,\n",
    "            )\n",
    "            run.log(info)\n",
    "            trained_runs_ids.append(run.id)\n",
    "    print(trained_runs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory from previous models\n",
    "del bi_lstm\n",
    "del bi_gru\n",
    "del double_bi_lstm\n",
    "del double_dense\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Loading Trained Models\n",
    "Once models have been trained and their \"best\" weights have been saved, we can load them back to evaluate their performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ids = ['1r268con', '13s0tew7', 'mszgpyn0', '36hs34rs']\n",
    "weighted_acc_ids = ['3m0ggwu5', '3owjlhkw', '3qw2e1fu', '3st72i2q']\n",
    "\n",
    "\n",
    "def load_models(ids: List[str]) -> Dict[str, tf.keras.Model]:\n",
    "    \"\"\"Load keras models and training info from wandb.\n",
    "\n",
    "    Args:\n",
    "        ids: list of wandb run ids\n",
    "\n",
    "    Returns:\n",
    "        models: mapping from model names to keras objects\n",
    "    \"\"\"\n",
    "    models = dict()\n",
    "    for id in ids:\n",
    "        uri = 'frantoman/NLP-POS-Tagging/' + id\n",
    "        api = wandb.Api()\n",
    "        run = api.run(uri)\n",
    "\n",
    "        mn = run.config['model-name']\n",
    "        print('Downloading', mn)\n",
    "        run.file(f'{mn}.h5').download(f'./models/', replace=True)\n",
    "\n",
    "        model = load_model(f'./models/{mn}.h5')\n",
    "        models[model.name] = model\n",
    "    return models\n",
    "\n",
    "\n",
    "models_unweighted = load_models(acc_ids)\n",
    "print('--------------------------')\n",
    "models_weighted = load_models(weighted_acc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Evaluation functions\n",
    "Now we define some functions to evaluate the model on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(x, y, p) -> pd.DataFrame:\n",
    "    \"\"\"Convert input, ground truth and prediction of a model to a dataframe\"\"\"\n",
    "    # Define a mapping int->token and int->tag\n",
    "    int_to_tok = {v: k for k, v in word_index.items()}\n",
    "    int_to_tag = {v: k for k, v in tag_to_int.items()}\n",
    "    convert_tok = np.vectorize(int_to_tok.get)\n",
    "    convert_tag = np.vectorize(lambda i: int_to_tag.get(i, 'PAD'))\n",
    "\n",
    "    # Convert one-hot encodings to integer indexes\n",
    "    y = np.argmax(y, axis=-1)\n",
    "    p = np.argmax(p, axis=-1)\n",
    "\n",
    "    # Convert integer indexes to the corresponding token/tag, skipping padding\n",
    "    raw_df = dict(token=[], tag_true=[], tag_pred=[])\n",
    "    for zx, zy, zp in zip(x, y, p):\n",
    "        pad_mask = zx > 0\n",
    "        raw_df['token'] += convert_tok(zx[pad_mask]).tolist()\n",
    "        raw_df['tag_true'] += convert_tag(zy[pad_mask]).tolist()\n",
    "        raw_df['tag_pred'] += convert_tag(zp[pad_mask]).tolist()\n",
    "\n",
    "    # Note: the returned df will contain truncated sentences\n",
    "    df = pd.DataFrame(raw_df)\n",
    "    df['tag_true'] = df['tag_true'].astype('category')\n",
    "    df['tag_pred'] = df['tag_pred'].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def classif_report(pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Writes a report showing the main classification metrics\n",
    "    on a text file.\n",
    "\n",
    "    Args:\n",
    "        pred: dataframe with columns 'tag_true' and 'tag_pred'.\n",
    "        model_name: name of the model the dataset refers to.\n",
    "        weigthed: whether the model has been trained with weighted loss or not.\n",
    "    \"\"\"\n",
    "    # First, remove punctuation\n",
    "    punctuation = \"``|''|\\,|\\.|\\:|-LRB-|-RRB-\"\n",
    "    is_punct = pred['tag_true'].str.contains(punctuation)\n",
    "    pred = pred[~is_punct]\n",
    "\n",
    "    # Then calculate the classification report\n",
    "    cr = classification_report(\n",
    "        pred['tag_true'],\n",
    "        pred['tag_pred'],\n",
    "        labels=pred['tag_true'].unique(),\n",
    "        zero_division=0,\n",
    "        output_dict=True,\n",
    "    )\n",
    "    df = pd.DataFrame(cr).transpose().round(3)\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_model(model, x, y):\n",
    "    df_pred = to_dataframe(x, y, model.predict(x))\n",
    "    cr = classif_report(df_pred)\n",
    "    f1m = cr.loc['macro avg', 'f1-score']\n",
    "    return df_pred, cr, f1m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Weighted vs Unweighted class training\n",
    "In the training phase we tried solving the problem class unbalance by using sample weighting.  \n",
    "Now we evaluate if the sample weighting has had any effect on the model's performance, by using F1-Macro on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_evaluation = []\n",
    "for models, note in [\n",
    "    (models_unweighted, \"Un-Weighted\"),\n",
    "    (models_weighted, \"Weighted\"),\n",
    "]:\n",
    "    for model in models.values():\n",
    "        _, _, f1m = evaluate_model(model, x_val, y_val)\n",
    "        weight_evaluation.append(\n",
    "            dict(model_name=model.name,\n",
    "                 notes=note,\n",
    "                 f1_macro_val=f1m,\n",
    "                 model=model))\n",
    "\n",
    "weight_evaluation = pd.DataFrame(weight_evaluation).sort_values(\n",
    "    by=['notes', 'f1_macro_val'], ascending=False)\n",
    "display(weight_evaluation.loc[:, weight_evaluation.columns != 'model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, there is not much difference between the weighted and unweighted training. Nevetheless, a slight increase in F1-Score is present. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Test Set Performance\n",
    "In the following, we are going to evaluate the test set performance of the two models that reached the highest f1 score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_eval = weight_evaluation.iloc[:2].copy().reset_index(drop=True)\n",
    "best_models_eval['f1_macro_test'] = [\n",
    "    evaluate_model(model, x_test, y_test)[2]\n",
    "    for model in best_models_eval.model.values\n",
    "]\n",
    "display(best_models_eval.loc[:, best_models_eval.columns != 'model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set performance exceeds the expectations with respect to the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Error Analysis\n",
    "In this section we analyze the errors made on the test set by the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_models_eval.sort_values(by=['f1_macro_val'],\n",
    "                                          ascending=False).loc[0, 'model']\n",
    "test_pred, report, _ = evaluate_model(best_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Problematic classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'f1-score'\n",
    "class_f1 = report[:-3].sort_values(by=metric)\n",
    "class_f1['support'] = -class_f1['support'] / sum(\n",
    "    class_f1['support'])  # Norm+sign\n",
    "fig = px.histogram(class_f1, x=class_f1.index, y=[metric, 'support'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that f1 scores are not correlated with their support. Moreover, classes with high support such as `NN` and `NNP` are not the best scoring, while the worse class of all is `NNPS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df: pd.DataFrame, threshold: float = 1.0):\n",
    "    # Extract true tags and predictions\n",
    "    corr, pred = df['tag_true'], df['tag_pred']\n",
    "    labels = list(set(corr) | set(pred))  # Get tag names\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf = confusion_matrix(corr, pred, labels=labels, normalize='true')\n",
    "\n",
    "    # Threshold the diagonal, to keep only problematic classes\n",
    "    diag = conf.diagonal() <= threshold\n",
    "    conf = conf[np.ix_(diag, diag)]\n",
    "    labels = np.array(labels)[diag]\n",
    "\n",
    "    # Plot matrix\n",
    "    fig = px.imshow(conf,\n",
    "                    x=labels,\n",
    "                    y=labels,\n",
    "                    labels={\n",
    "                        'x': 'Predicted Tag',\n",
    "                        'y': 'True Tag',\n",
    "                        'color': 'Value'\n",
    "                    })\n",
    "    fig.update(layout_coloraxis_showscale=False)\n",
    "    fig.update_layout(width=600, height=600)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(test_pred, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - OOV Words\n",
    "Lets see if OOV words have an impact on the f1 macro score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_tok_subset(df, tokens):\n",
    "    mask = df['token'].apply(lambda t: t not in tokens)\n",
    "    df = df[mask]\n",
    "    cr = classif_report(df)\n",
    "    return cr.loc['macro avg', 'f1-score']\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "F1-Macro on test set:\n",
    "  - All tokens:           {get_f1_tok_subset(test_pred, [])}\n",
    "  - Tokens not in GloVe:  {get_f1_tok_subset(test_pred, glove_keys)}\n",
    "  - Tokens not in V1:     {get_f1_tok_subset(test_pred, oov1)}\n",
    "  - Tokens not in V2:     {get_f1_tok_subset(test_pred, oov2)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: scrivere qualcosa su questo ^"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
