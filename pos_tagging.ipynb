{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3B9biLeVvc1"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main framework\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pathlib\n",
    "\n",
    "# Notebook visualization\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Seed initialization\n",
    "random.seed(0)\n",
    "\n",
    "# Typing\n",
    "from typing import Set\n",
    "\n",
    "# For GloVe wrapper\n",
    "!pip install gensim -U\n",
    "import gensim\n",
    "from gensim import downloader as gensloader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## 1 - Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### 1.1 - Data loading\n",
    "First, we load the dataset and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './dependency_treebank'  # Change if dataset already present locally\n",
    "DATASET_URL = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not pathlib.Path(ds_path).exists():\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']:\n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI9ECEh3Vvc6"
   },
   "source": [
    "### 1.2 - GloVe loading\n",
    "Then, we load the GloVe embeddings (GloVe-50, to be precise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_TYPE = 'glove-wiki-gigaword-50'\n",
    "GLOVE_FILE = './glove/glove-wiki-gigaword-50.kv'\n",
    "\n",
    "\n",
    "def load_glove(gl_file: str, gl_type: str) -> KeyedVectors:\n",
    "    # Load local version\n",
    "    path = pathlib.Path(gl_file)\n",
    "    if path.exists():\n",
    "        return gensim.models.KeyedVectors.load(gl_file)\n",
    "\n",
    "    # Otherwise download and store glove\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    glove = gensloader.load(gl_type)\n",
    "    glove.save(gl_file)\n",
    "    return glove\n",
    "\n",
    "\n",
    "glove = load_glove(GLOVE_FILE, GLOVE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GloVe loading\n",
    "print(f'cat = {glove[\"cat\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlL7FRunWqwE"
   },
   "source": [
    "### 1.3 - Data visualization\n",
    "One of the most important ML tasks is getting familiar with the data in order to gain a deeper insight on their structure and nature.\n",
    "\n",
    "To do so, we define a function that displays tokens with their POS tags in a human-friendlier way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this could be put in its own file to keep things clean,\n",
    "#       but to upload just one notebook we instead code-golfed a bit :)\n",
    "\n",
    "# Define a mapping between POS tags, their meaning and some colors\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_map = {\n",
    "    'CC': ('Coordin. Conjunction', '#c18401'),\n",
    "    'TO': ('“to”', '#c18401'),\n",
    "    'DT': ('Determiner', '#c18401'),\n",
    "    'UH': ('Interjection', '#c18401'),\n",
    "    'EX': ('Existential ‘there', '#c18401'),\n",
    "    'MD': ('Modal can', '#c18401'),\n",
    "    'LS': ('List item marker', '#c18401'),\n",
    "    'IN': ('Preposition/sub-conj', '#c18401'),\n",
    "    'CD': ('Cardinal number', '#282828'),\n",
    "    'FW': ('Foreign word', '#282828'),\n",
    "    'NN': ('Noun, singular/mass', '#282828'),\n",
    "    'NNS': ('Noun, plural', '#282828'),\n",
    "    'NNP': ('Proper noun, singul.', '#282828'),\n",
    "    'NNPS': ('Proper noun, plural', '#282828'),\n",
    "    'JJ': ('Adjective', '#50a14f'),\n",
    "    'JJR': ('Adj. comparative ', '#50a14f'),\n",
    "    'JJS': ('Adj. superlative ', '#50a14f'),\n",
    "    'VB': ('Verb, base form', '#e45649'),\n",
    "    'VBD': ('Verb, past tense ', '#e45649'),\n",
    "    'VBG': ('Verb, gerund ', '#e45649'),\n",
    "    'VBN': ('Verb, past particip. ', '#e45649'),\n",
    "    'VBP': ('Verb, non-3sg pres', '#e45649'),\n",
    "    'VBZ': ('Verb, 3sg pres ', '#e45649'),\n",
    "    'WDT': ('Wh-determiner', '#4078f2'),\n",
    "    'WP': ('Wh-pronoun', '#4078f2'),\n",
    "    'WP$': (' Possessive wh-', '#4078f2'),\n",
    "    'WRB': ('Wh-adverb how', '#4078f2'),\n",
    "    'PDT': ('Predeterminer ', '#4078f2'),\n",
    "    'POS': ('Possessive ending', '#4078f2'),\n",
    "    'PP': ('Personal pronoun', '#4078f2'),\n",
    "    'PP$': (' Possessive pronoun ', '#4078f2'),\n",
    "    'RB': ('Adverb', '#a626a4'),\n",
    "    'RBR': ('Adverb, comparative', '#a626a4'),\n",
    "    'RBS': ('Adverb, superlative', '#a626a4'),\n",
    "    'RP': ('Particle', '#a626a4'),\n",
    "}\n",
    "tag_map = defaultdict(lambda: ('', '#282828'), tag_map)\n",
    "\n",
    "\n",
    "def display_pos_tagging(tokens: pd.Series,\n",
    "                        predicted_tags: pd.Series,\n",
    "                        correct_tags: pd.Series = None,\n",
    "                        limit=1000):\n",
    "    # If no correct tags are passed, we ignore the \"error highlighting\"\n",
    "    if correct_tags is None:\n",
    "        correct_tags = predicted_tags\n",
    "\n",
    "    # Limit the inputs\n",
    "    tokens = tokens[:limit]\n",
    "    predicted_tags = predicted_tags[:limit]\n",
    "    correct_tags = correct_tags[:limit]\n",
    "\n",
    "    # Iterate through tokens and tags, generating styled HTML based on the tags\n",
    "    html_sequence = []\n",
    "    for token, tag, correct in zip(tokens, predicted_tags, correct_tags):\n",
    "        tag_meaning = tag_map[tag][0]\n",
    "        err = 'pos-error' if tag != correct else ''\n",
    "        h = f'<div class=\"token {tag} {err}\">{token} <span class=\"tag\">[{tag}] {tag_meaning}</span></div>'\n",
    "        if tag == '.':\n",
    "            h += '<div class=\"separator\"/>'\n",
    "        html_sequence.append(h)\n",
    "    html_body = '<div class=\"pos-visualizer\">'\n",
    "    html_body += ' '.join(html_sequence) + '</div>'\n",
    "\n",
    "    # Generate the style (WARNING: CSS lies ahead)\n",
    "    html_style = \"\"\"\n",
    "\t<style>\n",
    "\t.pos-visualizer { padding: 32px; background-color: #FEFEFE; border-left:solid 1px grey;}\n",
    "\t.token { position:relative; display:inline-block; font-size:16px;}\n",
    "\t.token .tag { \n",
    "\t\tvisibility:hidden; width: 120px; text-align:center; position:absolute;\n",
    "\t\twidth: 160px; background-color: #282828; color: #fff; border-radius: 6px;\n",
    "\t\tz-index: 1; bottom: 100%; left: 50%; margin-left:-80px; font-size:12px;\n",
    "\t}\n",
    "\t.pos-error { text-decoration: underline solid #F94144;}\n",
    "\t.separator { margin-top:12px }\n",
    "\t.token:hover .tag { visibility:visible }\n",
    "\t\"\"\"\n",
    "    html_style += '\\n'.join((f'.{tag} {{color:{tag_map[tag][1]};}}'\n",
    "                             for tag in predicted_tags.unique()))\n",
    "    html_style += '</style>'\n",
    "\n",
    "    # Display the HTML in the cell's output\n",
    "    display(HTML(html_style + html_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample POS tagging (hover on text for tag meaning)\n",
    "predicted_example = dataset['tag'].copy()\n",
    "predicted_example[0:8] = 'CD'  # Wrong prediction example for the first 8 words\n",
    "display_pos_tagging(dataset['token'],\n",
    "                    predicted_example,\n",
    "                    dataset['tag'],\n",
    "                    limit=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### 1.4 - Pre-processing\n",
    "Our dataset is already relatively clean; however, one point that might be worth considering is how to handle lowercase conversions. Some tokens in our dataset will be intrinsically capitalized (e.g. proper nouns, the personal pronoun \"I\"), whereas some other will be capitalized only because they follow a period in the sentence they occur in.\n",
    "\n",
    "One might think of converting a token to lowercase based on its tag (e.g. if a token is a proper noun, keep it capitalized); however, to be fair, this could only be done on the training set, since in a real scenario test-set tags would be unknown.\n",
    "\n",
    "Anyway, all these considerations hold only if GloVe contains embeddings of capitalized words; if that's not the case, every word we keep as capitalized will be classified as OOV when matched with GloVe, even when their lowercase embedding actually exists.\n",
    "\n",
    "As it turns out, Glove does not encode capitalized words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_capitalized = len(list(filter(lambda w: w[0].isupper(), glove.key_to_index.keys())))\n",
    "\n",
    "print(f'GloVe-50 encodes {num_capitalized} capitalized words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kqy2eiLVvdC"
   },
   "source": [
    "Therefore, we will be forced to convert all tokens to lowercase.\n",
    "\n",
    "We are also interested to see which \"special\" tokens are encoded in GloVe, i.e. punctuation, quotation marks, and tokens such as \"-LRB-\" and \"-RRB-\", which in our dataset replace \"(\" and \")\", respectively.\n",
    "\n",
    "As it turns out, GloVe contains every special symbol we care about, except for tokens reserved to brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    *',.:;\"`$#£!%/?^-()[]{}_', \"''\", \"``\", \"--\", \"-LRB-\", \"-RRB-\", \"-LSB-\",\n",
    "    \"-RSB-\", \"-LCB-\", \"-RCB-\"\n",
    "]\n",
    "for st in special_tokens:\n",
    "    if st not in glove:\n",
    "        print(f\"GloVe does not contain token {st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2B391arVvdD"
   },
   "source": [
    "Based on the previous considerations, we convert all tokens to lowercase and replace \"-LRB\"-like symbols with the corresponding bracket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the brackets\n",
    "for token, bracket in [('-LRB-', '('), ('-RRB-', ')'), ('-LSB-', '['),\n",
    "                       ('-RSB-', ']'), ('-LCB-', '{'), ('-RCB-', '}')]:\n",
    "    dataset.loc[dataset.token == token, 'token'] = bracket\n",
    "\n",
    "# Convert dataset tokens to lowercase\n",
    "dataset.loc[:, 'token'] = dataset['token'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLuhT_wCVvdE"
   },
   "source": [
    "### 1.5 - Splitting\n",
    "After pre-processing the data, we can finally split the dataset into train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = dataset[dataset['document'].lt(100)]\n",
    "ds_val = dataset[dataset['document'].between(100, 149)].reset_index()\n",
    "ds_test = dataset[dataset['document'].gt(149)].reset_index()\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} tokens\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(ds_train)}\n",
    "    VALIDATION: {print_split(ds_val)}\n",
    "    TEST: {print_split(ds_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70PQNGtHVvdE"
   },
   "source": [
    "### 1.6 - OOV Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dii_iOaHI4t1"
   },
   "source": [
    "#### 1.6.1 - OOV Analysis\n",
    "First of all, let us take a look at how many Out-Of-Vocabulary tokens (w.r.t. GloVe) our dataset contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oov(tokens, embedding_keys):\n",
    "    return set(tokens) - set(embedding_keys)\n",
    "\n",
    "# NOTE _ TODO: gensim3 usa glove.vocab al posto di .key_to_index.keys()\n",
    "glove_keys = glove.key_to_index.keys()\n",
    "oov_train = get_oov(ds_train['token'].unique(), glove_keys)\n",
    "oov_val = get_oov(ds_val['token'].unique(), glove_keys)\n",
    "oov_test = get_oov(ds_test['token'].unique(), glove_keys)\n",
    "\n",
    "print_oov = lambda s, d: f\"{len(s)} [{len(s) / len(d['token'].unique()) * 100:.2f}%]\"\n",
    "print(f\"\"\"Number of OOV tokens in dataset:\n",
    "    TRAIN: {print_oov(oov_train, ds_train)}\n",
    "    VALIDATION: {print_oov(oov_val, ds_val)}\n",
    "    TEST: {print_oov(oov_test, ds_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y22qDFw_VvdG"
   },
   "source": [
    "However, from an experimental perspective, we should consider \"incremental\" OOV words, i.e. how OOVs would actually be identified in a real world scenario:\n",
    "* **Training OOVs:** training-set tokens which are not found in GloVe.\n",
    "* **Validation OOVs:** validation-set tokens which are not found in `union(`GloVe, Training OOVs`)`.\n",
    "* **Test OOVs:** test-set tokens which are not found in `union(`GloVe, Training OOVs, Validation OOVs`)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"Number of OOV tokens in dataset, considering INCREMENTAL OOV EMBEDDING:\n",
    "    TRAIN: {print_oov(oov_train, ds_train)}\n",
    "    VALIDATION: {print_oov(oov_val - oov_train, ds_val)}\n",
    "    TEST: {print_oov(oov_test - (oov_val | oov_train), ds_test)}\n",
    "    === TOTAL: {print_oov(oov_train | oov_val | oov_test, dataset)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJkqb0b9VvdH"
   },
   "source": [
    "#### 1.6.2 - Adding OOVs to GloVe\n",
    "We can now add OOV tokens to the GloVe vocabulary. Many strategies can be adopted to encode OOVs as vectors:\n",
    "1. Static embeddings with the same vector for all OOV tokens (e.g. zeros).\n",
    "2. Random embeddings. \n",
    "3. Computing an embedding as some statistic involving neighboring tokens (e.g. their mean).\n",
    "\n",
    "Two observations can guide us in the choice of an embedding strategy:\n",
    "* OOV tokens are not negligible (about 6% of the *total* dataset)\n",
    "* Our GloVe embeddings will not undergo further training, therefore fixed or random embedding values will not be refined during the training process.\n",
    "\n",
    "For the two reasons above, given an OOV token, we will compute its embedding as the mean of its left and right neighbors across all its occurrences throughout the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighbor_mean(oov_token: str, df: pd.DataFrame,\n",
    "                                 embeddings: KeyedVectors) -> np.ndarray:\n",
    "    # Find indexes where the oov token appears, and shift them by -1 +1\n",
    "    indexes = df.index[df['token'] == oov_token].values\n",
    "    indexes = np.concatenate((indexes - 1, indexes + 1))\n",
    "\n",
    "    # For each oov word index, look at the left and right until a word with embedding has been found\n",
    "    neighbor_embeddings = []\n",
    "    for idx in indexes:\n",
    "        for direction in (range(idx - 1, -1, -1), range(idx + 1, len(df))):\n",
    "            for i in direction:\n",
    "                tok = df['token'].iloc[i]\n",
    "                if tok not in embeddings:\n",
    "                    continue\n",
    "                vector = embeddings[tok]\n",
    "                neighbor_embeddings.append(vector)\n",
    "                break\n",
    "\n",
    "    return np.mean(neighbor_embeddings, axis=0)\n",
    "\n",
    "\n",
    "def fill_oov_embeddings(oov_tokens: Set, df: pd.DataFrame,\n",
    "                       embeddings: KeyedVectors) -> KeyedVectors:\n",
    "    # Clone the embedding (KeyedVectors does not have a clone method)\n",
    "    from copy import deepcopy\n",
    "    emb_filled = deepcopy(embeddings)\n",
    "\n",
    "    # Estimate the OOV embeddings\n",
    "    keys, values = [], []\n",
    "    for oov in oov_tokens:\n",
    "        vector = compute_neighbor_mean(oov, df, emb_filled)\n",
    "        keys.append(oov)\n",
    "        values.append(vector)\n",
    "    # Add the estimates to the embedding\n",
    "    emb_filled.add_vectors(keys, values)\n",
    "    return emb_filled\n",
    "\n",
    "\n",
    "emb_train = fill_oov_embeddings(oov_train, ds_train, glove)\n",
    "emb_val = fill_oov_embeddings(oov_val - oov_train, ds_val, emb_train)\n",
    "emb_test = fill_oov_embeddings(oov_test - oov_val - oov_train, ds_test, emb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding dimensions\n",
    "dim = len(glove['cat'])\n",
    "print(f'glove.shape     = ({len(glove)}, {dim})')\n",
    "print(f'emb_train.shape = ({len(emb_train)}, {dim})')\n",
    "print(f'emb_val.shape   = ({len(emb_val)}, {dim})')\n",
    "print(f'emb_val.shape   = ({len(emb_test)}, {dim})')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
