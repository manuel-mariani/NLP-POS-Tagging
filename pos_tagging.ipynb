{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQQcUC98GA_O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Types and type-annotations\n",
    "from typing import List, Dict\n",
    "from collections import OrderedDict\n",
    "\n",
    "# To store vocabulary as .json\n",
    "!pip install simplejson\n",
    "import simplejson as sj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### Data Loading\n",
    "First we load the data (downloading it if not present), and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1637449554559,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "Nu5i9VRDGA_S",
    "outputId": "a09a4a56-ec9d-4aca-b394-953aedeb5044"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./dependency_treebank\"\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not os.path.isdir(ds_path):\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']: \n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset[dataset['document'].lt(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### Data Pre-Processing\n",
    "The dataset does not need much of a cleanup: the only pre-processing we need to perform is converting tokens to lowercase, so that we can create a vocabulary without ending up with two entries for the same word.\n",
    "\n",
    "However, we need to distinguish between words that are inherently capitalized (e.g. proper nouns) and those that are so just because they follow a period.\n",
    "\n",
    "First of all, let us check which kinds of tags produce capitalized words, and how many those words are for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1637447288826,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "VlFHftXOFjHI",
    "outputId": "84d02ef3-cbf1-4865-ca56-cc2631717807"
   },
   "outputs": [],
   "source": [
    "dataset[dataset['token'].str[0].str.isupper()].groupby('tag').size().reset_index(name='capitalized counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCnFAL4dOl9N"
   },
   "source": [
    "The most meaningful tags are (see [here](https://sites.google.com/site/partofspeechhelp/)):\n",
    "* `NNP`: Proper Nouns (Singular)\n",
    "* `NNPS`: Proper Nouns (Plural)\n",
    "* `PRP`: Personal Pronouns\n",
    "\n",
    "The weird ones are:\n",
    "* `$`: Dollar mark\n",
    "* `,`: Non-full stop break punctuation marks for the sentence\n",
    "\n",
    "Proper nouns are always capitalized, and we should probably leave them as such, as in this case capitalization and tag are tigthly linked to each other.\n",
    "\n",
    "Personal pronouns are meaningfully capitalized only in the case of \"I\", which is capitalized no matter where it occurs in a sentence. We should therefore keep \"I\" as it is and convert the other pronouns to lowecase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCvXBlpOU-gi"
   },
   "source": [
    "When is `$` capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1637441306787,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "8FfL-607BOV9",
    "outputId": "4229aa74-2515-428c-8914-0678a4291ca3"
   },
   "outputs": [],
   "source": [
    "dataset[dataset['tag']=='$'].groupby('token').size().reset_index(name='\"$\"-tag counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXFwyFG4L4us"
   },
   "source": [
    "The `$` tag is attached not only to the dollar symbol, but also to \"C\\$\" (Canadian dollars) and \"US$\" (United States dollar), which are capitalized strings. It might make sense to leave them uppercase, as they are in some sense a label denoting a special symbol. In any case, there are just 6 of them in the whole dataset, so it probably does not matter that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ILEUjrpk5C0"
   },
   "source": [
    "When is `,` capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1637449258711,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "M5XayQR6d9LH",
    "outputId": "14b32cb7-f312-49ec-ba38-5ffc65cd6793"
   },
   "outputs": [],
   "source": [
    "tmp = dataset[dataset['token'].str[0].str.isupper()]\n",
    "tmp[tmp['tag'] == ',']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St5bYIJSlPM6"
   },
   "source": [
    "No idea what \"Wa\" means, possibly a typo/labeling mistake. Anyway, being a single instance, it really does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B2Vsj-IGA_S"
   },
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1637274720898,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "VjSpMtJzGA_T",
    "outputId": "e624f533-5eb0-49a7-ef0d-d5c9eb428b75"
   },
   "outputs": [],
   "source": [
    "train_ds = dataset[dataset['document'].lt(100)]\n",
    "validation_ds = dataset[dataset['document'].between(100, 149)]\n",
    "test_ds = dataset[dataset['document'].gt(149)]\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} samples\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(train_ds)}\n",
    "    VALIDATION: {print_split(validation_ds)}\n",
    "    TEST: {print_split(test_ds)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW8zQ7RuGA_U"
   },
   "source": [
    "### Vocaboulary Creation\n",
    "> TODO: lowercase conversion before vocabulaty creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLbXKgAmLeE3"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str],\n",
    "                                           Dict[str, int],\n",
    "                                           List[str]):\n",
    "    \"\"\"Given a dataset, builds the corresponding token vocabulary.\n",
    "    The vocabulary starts from index 1 so as to allow the 0 slot to be reserved to the padding token.\n",
    "\n",
    "    Args:\n",
    "        df: dataset, assumed to have a 'token' column.\n",
    "\n",
    "    Returns:\n",
    "        idx_to_pos: token vocabulary, i.e. from index to token.\n",
    "        pos_to_idx: inverse token vocabulary, i.e. from token to index.\n",
    "        pos_listing: list of unique tokens that build up the vocabulary.\n",
    "    \"\"\"\n",
    "    idx_to_tok = OrderedDict()\n",
    "    tok_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 1\n",
    "    for tok in df['token']:\n",
    "        if tok not in tok_to_idx:\n",
    "            tok_to_idx[tok] = curr_idx\n",
    "            idx_to_tok[curr_idx] = tok\n",
    "            curr_idx += 1\n",
    "\n",
    "    tok_listing = list(idx_to_tok.values())\n",
    "\n",
    "    return idx_to_tok, tok_to_idx, tok_listing\n",
    "\n",
    "\n",
    "idx_to_tok, tok_to_idx, tok_listing = build_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fshhmnqnx_Mt"
   },
   "source": [
    "Once the vocabulary is built, we perform some sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK-x0Jf1mp6j"
   },
   "outputs": [],
   "source": [
    "assert len(idx_to_tok) == len(tok_to_idx)\n",
    "assert len(idx_to_tok) == len(tok_listing)\n",
    "\n",
    "for i in range(1, len(idx_to_tok) + 1):\n",
    "    assert idx_to_tok[i] in tok_to_idx\n",
    "    assert tok_to_idx[idx_to_tok[i]] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMdjicUeyGcn"
   },
   "source": [
    "And then save the vocabulary for a more detailed inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hs6V3gSZxjZb"
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(os.getcwd(), 'vocab.json')\n",
    "\n",
    "with open(vocab_path, 'w') as f:\n",
    "    sj.dump(idx_to_tok, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pos_tagging.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "2cdbff7efa5df5bfb11dcd583c56ac0f576766c47f09cab205fa9f1668d16c1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
