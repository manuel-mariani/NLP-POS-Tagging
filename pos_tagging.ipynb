{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Summary and Structure\n",
    "The notebook is divided as follows:\n",
    "\n",
    "1. **Data Pipeline**  \n",
    "    1.1 **Data loading**: load/download the nltk corpus  \n",
    "    1.2 **GloVe loading**: load/download the GloVe embeddings  \n",
    "    1.3 **Data visualization**: visualize the dataset in a human-readable way  \n",
    "    1.4 **Data cleanup**: clear the data from errors, handle capitalized words  \n",
    "    1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main framework\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# To store vocabulary as .json\n",
    "!pip install simplejson\n",
    "import simplejson\n",
    "\n",
    "# Notebook visualization\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Seed initialization\n",
    "random.seed(0)\n",
    "\n",
    "# Typing\n",
    "from typing import Set\n",
    "\n",
    "# For GloVe wrapper\n",
    "import gensim\n",
    "from gensim import downloader as gensloader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## 1 - Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### 1.1 - Data loading\n",
    "First we load the nltk corpus (downloading it if not present), and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./dependency_treebank\"  # Change if dataset already present locally\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not pathlib.Path(ds_path).exists():\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']:\n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - GloVe loading\n",
    "Now we load the GloVe embeddings (if present locally) or download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_TYPE = 'glove-wiki-gigaword-50'\n",
    "GLOVE_FILE = './glove/glove-wiki-gigaword-50.kv'\n",
    "\n",
    "\n",
    "def load_glove(gl_file: str, gl_type: str) -> KeyedVectors:\n",
    "    # Load local version\n",
    "    path = pathlib.Path(gl_file)\n",
    "    if path.exists():\n",
    "        return gensim.models.KeyedVectors.load(gl_file)\n",
    "\n",
    "    # Otherwise download and store glove\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    glove = gensloader.load(gl_type)\n",
    "    glove.save(gl_file)\n",
    "    return glove\n",
    "\n",
    "\n",
    "glove_embedding = load_glove(GLOVE_FILE, GLOVE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlL7FRunWqwE"
   },
   "source": [
    "### 1.3 - Data visualization\n",
    "One of the most important ML tasks is to gain familiarity with the data, to gain a deeper insight on the dataset. \n",
    "\n",
    "To do so, we define a function that displays a series of tokens and its POS tagging (predicted or from the corpus) in a more human-readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this could be put in its own file to keep things clean,\n",
    "#       but to upload just one notebook we instead code-golfed a bit :)\n",
    "\n",
    "# Define a mapping between POS tags, their meaning and some colors\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_map = {\n",
    "    'CC': ('Coordin. Conjunction', '#c18401'),\n",
    "    'TO': ('“to”', '#c18401'),\n",
    "    'DT': ('Determiner', '#c18401'),\n",
    "    'UH': ('Interjection', '#c18401'),\n",
    "    'EX': ('Existential ‘there', '#c18401'),\n",
    "    'MD': ('Modal can', '#c18401'),\n",
    "    'LS': ('List item marker', '#c18401'),\n",
    "    'IN': ('Preposition/sub-conj', '#c18401'),\n",
    "    'CD': ('Cardinal number', '#282828'),\n",
    "    'FW': ('Foreign word', '#282828'),\n",
    "    'NN': ('Noun, singular/mass', '#282828'),\n",
    "    'NNS': ('Noun, plural', '#282828'),\n",
    "    'NNP': ('Proper noun, singul.', '#282828'),\n",
    "    'NNPS': ('Proper noun, plural', '#282828'),\n",
    "    'JJ': ('Adjective', '#50a14f'),\n",
    "    'JJR': ('Adj. comparative ', '#50a14f'),\n",
    "    'JJS': ('Adj. superlative ', '#50a14f'),\n",
    "    'VB': ('Verb, base form', '#e45649'),\n",
    "    'VBD': ('Verb, past tense ', '#e45649'),\n",
    "    'VBG': ('Verb, gerund ', '#e45649'),\n",
    "    'VBN': ('Verb, past particip. ', '#e45649'),\n",
    "    'VBP': ('Verb, non-3sg pres', '#e45649'),\n",
    "    'VBZ': ('Verb, 3sg pres ', '#e45649'),\n",
    "    'WDT': ('Wh-determiner', '#4078f2'),\n",
    "    'WP': ('Wh-pronoun', '#4078f2'),\n",
    "    'WP$': (' Possessive wh-', '#4078f2'),\n",
    "    'WRB': ('Wh-adverb how', '#4078f2'),\n",
    "    'PDT': ('Predeterminer ', '#4078f2'),\n",
    "    'POS': ('Possessive ending', '#4078f2'),\n",
    "    'PP': ('Personal pronoun', '#4078f2'),\n",
    "    'PP$': (' Possessive pronoun ', '#4078f2'),\n",
    "    'RB': ('Adverb', '#a626a4'),\n",
    "    'RBR': ('Adverb, comparative', '#a626a4'),\n",
    "    'RBS': ('Adverb, superlative', '#a626a4'),\n",
    "    'RP': ('Particle', '#a626a4'),\n",
    "}\n",
    "tag_map = defaultdict(lambda: ('', '#282828'), tag_map)\n",
    "\n",
    "\n",
    "def display_pos_tagging(tokens: pd.Series,\n",
    "                        predicted_tags: pd.Series,\n",
    "                        correct_tags: pd.Series = None,\n",
    "                        limit=1000):\n",
    "    # If no correct tags are passed, we ignore the \"error highlighting\"\n",
    "    if correct_tags is None:\n",
    "        correct_tags = predicted_tags\n",
    "\n",
    "    # Limit the inputs\n",
    "    tokens = tokens[:limit]\n",
    "    predicted_tags = predicted_tags[:limit]\n",
    "    correct_tags = correct_tags[:limit]\n",
    "\n",
    "    # Iterate through tokens and tags, generating styled HTML based on the tags\n",
    "    html_sequence = []\n",
    "    for token, tag, correct in zip(tokens, predicted_tags, correct_tags):\n",
    "        tag_meaning = tag_map[tag][0]\n",
    "        err = 'pos-error' if tag != correct else ''\n",
    "        h = f'<div class=\"token {tag} {err}\">{token} <span class=\"tag\">[{tag}] {tag_meaning}</span></div>'\n",
    "        if tag == '.':\n",
    "            h += '<div class=\"separator\"/>'\n",
    "        html_sequence.append(h)\n",
    "    html_body = '<div class=\"pos-visualizer\">'\n",
    "    html_body += ' '.join(html_sequence) + '</div>'\n",
    "\n",
    "    # Generate the style (WARNING: CSS lies ahead)\n",
    "    html_style = \"\"\"\n",
    "\t<style>\n",
    "\t.pos-visualizer { padding: 32px; background-color: #FEFEFE; border-left:solid 1px grey;}\n",
    "\t.token { position:relative; display:inline-block; font-size:16px;}\n",
    "\t.token .tag { \n",
    "\t\tvisibility:hidden; width: 120px; text-align:center; position:absolute;\n",
    "\t\twidth: 160px; background-color: #282828; color: #fff; border-radius: 6px;\n",
    "\t\tz-index: 1; bottom: 100%; left: 50%; margin-left:-80px; font-size:12px;\n",
    "\t}\n",
    "\t.pos-error { text-decoration: underline solid #F94144;}\n",
    "\t.separator { margin-top:12px }\n",
    "\t.token:hover .tag { visibility:visible }\n",
    "\t\"\"\"\n",
    "    html_style += '\\n'.join((f'.{tag} {{color:{tag_map[tag][1]};}}'\n",
    "                             for tag in predicted_tags.unique()))\n",
    "    html_style += '</style>'\n",
    "\n",
    "    # Display the HTML in the cell's output\n",
    "    display(HTML(html_style + html_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample POS tagging (hover on text for tag meaning)\n",
    "predicted_example = dataset['tag'].copy()\n",
    "predicted_example[0:8] = 'CD'  # Wrong prediction example for the first 8 words\n",
    "display_pos_tagging(dataset['token'],\n",
    "                    predicted_example,\n",
    "                    dataset['tag'],\n",
    "                    limit=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### 1.4 - Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 - Analysis\n",
    "First of all, let's check if the GloVe embedded words contains just lowercase words or also capitalized or uppercase ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_upper = lambda w: not w.islower() and w.isalpha()\n",
    "glove_keys = glove_embedding.key_to_index.keys()\n",
    "uppercase_keys = list(filter(is_upper, glove_keys))\n",
    "\n",
    "# print(uppercase_keys)\n",
    "print(\n",
    "    f\"There are {len(uppercase_keys)} not lowercase tokens (that do not contain numbers or symbols) in GloVe keys\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our GloVe contains 128 keys that are not lowercase (and are only letters). Upon further inspection, those keys contain only non-latin characters strings, from languages such as Chinese, Arabic and others.\n",
    "\n",
    "Now we take a look at the uppercase words inside our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_tokens = list(filter(is_upper, dataset['token'].unique()))\n",
    "print(\n",
    "    f\"There are {len(uppercase_tokens)} uppercase alpha tokens in the corpus\")\n",
    "\n",
    "uppercase_tags = dataset[dataset['token'].str[0].str.isupper()]\\\n",
    "        .groupby('tag')\\\n",
    "        .size()\\\n",
    "        .sort_values(ascending=False)\\\n",
    "        .reset_index(name='capitalized counts')\n",
    "uppercase_tags.head()  # Display just the head for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains about 3000 uppercase (letters only) tokens, which are mostly latin. Those tokens come mostly from proper nouns, determiners, pronouns and generally words that come after a full stop (.)\n",
    "\n",
    "For example, personal pronouns are meaningfully capitalized only in the case of \"I\", which is capitalized no matter where it occurs in a sentence.\n",
    "\n",
    "A couple of weird uppercase tokens are:\n",
    "* `$`: Dollar mark\n",
    "* `,`: Non-full stop break punctuation marks for the sentence\n",
    "\n",
    "Lets see when `$` and `,` are capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['tag'] == '$'].groupby('token').size().reset_index(\n",
    "    name='\"$\"-tag counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `$` tag is attached not only to the dollar symbol, but also to \"C\\$\" (Canadian dollars) and \"US\\$\" (United States dollar), which are capitalized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dataset[dataset['token'].str[0].str.isupper()]\n",
    "tmp[tmp['tag'] == ',']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does \"Wa\" mean? and why is it tagged as `,`? Let's see it in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_sentences = dataset[dataset['token'].eq('Wa')]['sentence']\n",
    "wa_df = dataset[dataset['sentence'].isin(wa_sentences)]\n",
    "display_pos_tagging(wa_df['token'], wa_df['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text snipped above provides an insight on the meaning of the word \"Wa\". Also, it allows us to see that the single instance in which \"Wa\" is tagged as `,` is a labeling mistake, as the other occurrences are tagged as `NNP`. \n",
    "\n",
    "This outlier also indicates that the corpus contains some misclassified tokens. We can ignore this issue, as we can assume that labeling mistakes are very rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see which \"special\" tokens are contained in the embedding. Special tokens include punctuation, quotes, symbols and the `-LRB-`, `-RRB-`, `-LSB-` etc... parenthesis-brackets tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    *',.:;\"`$#£!%/?^-()[]{}_', \"''\", \"``\", \"--\", \"-LRB-\", \"-RRB-\", \"-LSB-\",\n",
    "    \"-RSB-\", \"-LCB-\", \"-RCB-\"\n",
    "]\n",
    "for st in special_tokens:\n",
    "    if st not in glove_keys:\n",
    "        print(f\"GloVe does not contain token {st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the embedding contains most special characters, except the `-XYB-` tokens which are \"artifacts\" from how the treebank has been constructed. We can safely convert those into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN64zPLlAMYI"
   },
   "source": [
    "#### 1.4.2 - Text Cleaning\n",
    "##### Lowercase\n",
    "We considered a couple of options for **cleaning the dataset from capitalized / uppercase tokens**:  \n",
    "1. Make ALL tokens lowercase  \n",
    "2. Make all tokens lowercase, except personal nouns and \"I\" pronouns.\n",
    "\n",
    "Both approaches come with some **issues**:\n",
    "1. By making all tokens lowercase, the model *\"loses information\"* about proper nouns and other kind of tokens.  \n",
    "2. This approach keeps information about those tags but has one big caveat: *we modify the test set* using the \"correct `tag`\", which is information that in real applications wouldn't be available, more so it's the thing we are trying to predict!\n",
    "\n",
    "Another alternative is, instead of cleaning the dataset beforehand, we can handle the upper and lower cases directly in the model's **embedding**.  \n",
    "If the GloVe keys were to contain also uppercase words, we could use a two-pass lookup in the Glove (first for the raw token and then for its lowercase version).   \n",
    "But since the GloVe keys contain only lowercase (latin) words, we can safely do only one pass (looking only at the lowercase word), even though performing two table accesses instead of one does not impact significantly on the performance.  \n",
    "The main advantage of handling lower/upper cases inside the embedding is that the tokens are not modified, so that we can output the documents in the exact form as they are provided, without the need of keeping a separate data structure for storing the correct capitalized documents.  \n",
    "Its disadvantage is training speed, since words would need to be converted multiple times during training which slows down the process. \n",
    "\n",
    "Overall, the best option in our case is to simply convert the dataset to lowercase, since our objective is just to perform Part Of Speech tagging.\n",
    "\n",
    "##### Brackets\n",
    "As said previously, it is safe to convert all `-XYB-` tokens, even inside the test set, since to perform the conversion we are just using the token itself and not the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the brackets\n",
    "for token, bracket in [('-LRB-', '('), ('-RRB-', ')'), ('-LSB-', '['),\n",
    "                       ('-RSB-', ']'), ('-LCB-', '{'), ('-RCB-', '}')]:\n",
    "    dataset.loc[dataset.token == token, 'token'] = bracket\n",
    "    # dataset.where(dataset['token'].ne(token), bracket, inplace=True)\n",
    "\n",
    "# Convert the dataset tokens into lowercase\n",
    "dataset.loc[:, 'token'] = dataset['token'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Splitting\n",
    "After pre-processing the data, we can finally split the dataset into train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset[dataset['document'].lt(100)]\n",
    "validation_ds = dataset[dataset['document'].between(100, 149)].reset_index()\n",
    "test_ds = dataset[dataset['document'].gt(149)].reset_index()\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} tokens\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(train_ds)}\n",
    "    VALIDATION: {print_split(validation_ds)}\n",
    "    TEST: {print_split(test_ds)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - OOV Handling\n",
    "\n",
    "#### 1.6.1 - OOV analysis\n",
    "First of all, lets take a look at how many Out Of Vocabulary terms we have in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oov(tokens, embedding_keys):\n",
    "    return set(tokens) - set(embedding_keys)\n",
    "\n",
    "\n",
    "oov_train = get_oov(train_ds['token'].unique(), glove_keys)\n",
    "oov_valid = get_oov(validation_ds['token'].unique(), glove_keys)\n",
    "oov_test = get_oov(test_ds['token'].unique(), glove_keys)\n",
    "\n",
    "print_oov = lambda s, d: f\"{len(s)} [{len(s) / len(d['token'].unique()) * 100:.2f}%]\"\n",
    "print(f\"\"\"Number of OOV tokens in dataset:\n",
    "    TRAIN: {print_oov(oov_train, train_ds)}\n",
    "    VALIDATION: {print_oov(oov_valid, validation_ds)}\n",
    "    TEST: {print_oov(oov_test, test_ds)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the **\"incremental\"** OOV words, meaning that for the **training** set the OOV remain the *same*, for the **validation** we consider the previously *training-oov-terms as present in the vocabulary*, and finally for the **test** set *both the training and validation oov are present*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"Number of OOV tokens in dataset, considering INCREMENTAL OOV EMBEDDING:\n",
    "    TRAIN: {print_oov(oov_train, train_ds)}\n",
    "    VALIDATION: {print_oov(oov_valid - oov_train, validation_ds)}\n",
    "    TEST: {print_oov(oov_test - (oov_valid | oov_train), test_ds)}\n",
    "    === TOTAL: {print_oov(oov_train | oov_valid | oov_test, dataset)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 - Filling the embedding\n",
    "We can now fill the GloVe vocabulary with the out of vocabulary tokens. We considered many strategies:\n",
    "1. Static embedding with the same vector for all OOV tokens (eg zeros)  \n",
    "2. Random embedding  \n",
    "3. Estimating the OOV embedding by considering the embedding of neighboring words in the corpus and performing algebraic operations (eg mean)\n",
    "\n",
    "Since the OOV tokens are not negligible (about 6% of the *total* dataset), we opted for option number 3\n",
    "\n",
    "NOTE: A slight variation of (3) would be to do a form of \"online learning\", where after the training OOV terms are computed, their estimate is refined using the data present in the validation/testing dataset, using some sort of weighted average between the current learnt embedding and the estimate using the other dataset. This requires more bookkeeping so for brevity it has not been chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X Francesco:\n",
    "ci stanno due funzioni che calcolano la media\n",
    "- la prima se non ha abbastanza vicini restituisce uno dei due oppure un vettore a caso\n",
    "- la seconda scorre finchè non trova un embedding valido\n",
    "TODO: REMOVE THIS COMMENT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_neighbor_mean(oov_token: str, df: pd.DataFrame,\n",
    "                          embedding: KeyedVectors) -> np.ndarray:\n",
    "    # Find indexes where the oov token appears, and shift them by -1 +1\n",
    "    indexes = df.index[df['token'] == oov_token].values\n",
    "    indexes = np.concatenate((indexes - 1, indexes + 1))\n",
    "    # Check for out of bounds: (very unnecessary)\n",
    "    indexes = indexes[(indexes >= 0) | (indexes <= len(df))]\n",
    "\n",
    "    # Save all neighbors embeddings in a list, only if embedding is found\n",
    "    neighbor_tokens = df['token'].iloc[indexes]\n",
    "    neighbor_embeddings = []\n",
    "    for nt in neighbor_tokens:\n",
    "        if nt in embedding:\n",
    "            neighbor_embeddings.append(embedding[nt])\n",
    "\n",
    "    # If we have enough elements, return their mean\n",
    "    if len(neighbor_embeddings) > 1:\n",
    "        return np.mean(neighbor_embeddings, axis=0)\n",
    "\n",
    "    # If we have just one element, return it\n",
    "    if len(neighbor_embeddings) == 1:\n",
    "        return neighbor_embeddings[0]\n",
    "\n",
    "    # Otherwise, we don't have any encoded neighbors, so we generate a random vector\n",
    "    print(\"0 NB\", oov_token)\n",
    "    return np.random.uniform(-0.05, 0.05, size=embedding['0'].size)\n",
    "\n",
    "\n",
    "def robust_compute_neighbor_mean(oov_token: str, df: pd.DataFrame,\n",
    "                                 embedding: KeyedVectors) -> np.ndarray:\n",
    "    # Find indexes where the oov token appears, and shift them by -1 +1\n",
    "    indexes = df.index[df['token'] == oov_token].values\n",
    "    indexes = np.concatenate((indexes - 1, indexes + 1))\n",
    "\n",
    "    # For each oov word index, look at the left and right until a word with embedding has been found\n",
    "    # AH YES, I LOVE C-LIKE CODE IN PYTHON TODO: REMOVE THIS COMMENT\n",
    "    neighbor_embeddings = []\n",
    "    for idx in indexes:\n",
    "        for direction in (range(idx - 1, -1, -1), range(idx + 1, len(df))):\n",
    "            for i in direction:\n",
    "                tok = df['token'].iloc[i]\n",
    "                if tok not in embedding:\n",
    "                    continue\n",
    "                emb = embedding[tok]\n",
    "                neighbor_embeddings.append(emb)\n",
    "                break\n",
    "\n",
    "    return np.mean(neighbor_embeddings, axis=0)\n",
    "\n",
    "\n",
    "def fill_oov_embedding(oov_tokens: Set, df: pd.DataFrame,\n",
    "                       embedding: KeyedVectors) -> KeyedVectors:\n",
    "    # Clone the embedding (KeyedVectors does not have a clone method)\n",
    "    from copy import deepcopy\n",
    "    filled = deepcopy(embedding)\n",
    "\n",
    "    # Estimate the OOV embeddings\n",
    "    keys, values = [], []\n",
    "    for oov in oov_tokens:\n",
    "        estimated_embedding = robust_compute_neighbor_mean(oov, df, filled)\n",
    "        keys.append(oov)\n",
    "        values.append(estimated_embedding)\n",
    "    # print(list(zip(keys, values)))\n",
    "    # Add the estimates to the embedding\n",
    "    filled.add_vectors(keys, values)\n",
    "    return filled\n",
    "\n",
    "\n",
    "training_embedding = fill_oov_embedding(oov_train, train_ds, glove_embedding)\n",
    "validation_embedding = fill_oov_embedding(oov_valid - oov_train, validation_ds,\n",
    "                                          training_embedding)\n",
    "testing_embedding = fill_oov_embedding(oov_test - oov_valid - oov_train,\n",
    "                                       test_ds, validation_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW8zQ7RuGA_U"
   },
   "source": [
    "### Vocabulary Creation\n",
    "> TODO: it would probably be better to define a ``Keras.Tokenizer`` wrapper that handles vocabulary creation, embedding and OOV tokens alltogether, as done is Section 6.3 of `Tutorial2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "tokenizer.fit_on_texts(train_ds['token'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMdjicUeyGcn"
   },
   "source": [
    "Save the vocabulary for a more detailed inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(os.getcwd(), 'vocab.json')\n",
    "\n",
    "with open(vocab_path, 'w') as f:\n",
    "    simplejson.dump(tokenizer.word_index, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
