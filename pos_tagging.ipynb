{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Types and type-annotations\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "# To store vocabulary as .json\n",
    "!pip install simplejson\n",
    "import simplejson as sj\n",
    "\n",
    "# Notebook visualization\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Seed initialization\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### Data Loading\n",
    "First we load the data (downloading it if not present), and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./dependency_treebank\"\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not os.path.isdir(ds_path):\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']:\n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset[dataset['document'].lt(1)]  # Display the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### Data Pre-Processing\n",
    "The dataset does not need much of a cleanup: the only pre-processing we need to perform is converting tokens to lowercase, so that we can create a vocabulary without ending up with two entries for the same word.\n",
    "\n",
    "However, we need to distinguish between words that are inherently capitalized (e.g. proper nouns) and those that are so just because they follow a period.\n",
    "\n",
    "First of all, let us check which kinds of tags produce capitalized words, and how many those words are for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['token'].str[0].str.isupper()].groupby(\n",
    "    'tag').size().reset_index(name='capitalized counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCnFAL4dOl9N"
   },
   "source": [
    "The most meaningful tags are (see [here](https://sites.google.com/site/partofspeechhelp/)):\n",
    "* `NNP`: Proper Nouns (Singular)\n",
    "* `NNPS`: Proper Nouns (Plural)\n",
    "* `PRP`: Personal Pronouns\n",
    "\n",
    "The weird ones are:\n",
    "* `$`: Dollar mark\n",
    "* `,`: Non-full stop break punctuation marks for the sentence\n",
    "\n",
    "Proper nouns are always capitalized, and we should probably leave them as such, as in this case capitalization and tag are tigthly linked to each other.\n",
    "\n",
    "Personal pronouns are meaningfully capitalized only in the case of \"I\", which is capitalized no matter where it occurs in a sentence. We should therefore keep \"I\" as it is and convert the other pronouns to lowecase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCvXBlpOU-gi"
   },
   "source": [
    "When is `$` capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['tag'] == '$'].groupby('token').size().reset_index(\n",
    "    name='\"$\"-tag counts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXFwyFG4L4us"
   },
   "source": [
    "The `$` tag is attached not only to the dollar symbol, but also to \"C\\$\" (Canadian dollars) and \"US$\" (United States dollar), which are capitalized strings. It might make sense to leave them uppercase, as they are in some sense a label denoting a special symbol. In any case, there are just 6 of them in the whole dataset, so it probably does not matter that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ILEUjrpk5C0"
   },
   "source": [
    "When is `,` capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dataset[dataset['token'].str[0].str.isupper()]\n",
    "tmp[tmp['tag'] == ',']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St5bYIJSlPM6"
   },
   "source": [
    "No idea what \"Wa\" means, possibly a typo/labeling mistake. Anyway, being a single instance, it really does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B2Vsj-IGA_S"
   },
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset[dataset['document'].lt(100)]\n",
    "validation_ds = dataset[dataset['document'].between(100, 149)]\n",
    "test_ds = dataset[dataset['document'].gt(149)]\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} samples\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(train_ds)}\n",
    "    VALIDATION: {print_split(validation_ds)}\n",
    "    TEST: {print_split(test_ds)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW8zQ7RuGA_U"
   },
   "source": [
    "### Vocabulary Creation\n",
    "> TODO: lowercase conversion before vocabulaty creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(\n",
    "        df: pd.DataFrame) -> Tuple[Dict[int, str], Dict[str, int], List[str]]:\n",
    "    \"\"\"Given a dataset, builds the corresponding token vocabulary.\n",
    "    The vocabulary starts from index 1 so as to allow the 0 slot to be reserved to the padding token.\n",
    "\n",
    "    Args:\n",
    "        df: dataset, assumed to have a 'token' column.\n",
    "\n",
    "    Returns:\n",
    "        idx_to_pos: token vocabulary, i.e. from index to token.\n",
    "        pos_to_idx: inverse token vocabulary, i.e. from token to index.\n",
    "        pos_listing: list of unique tokens that build up the vocabulary.\n",
    "    \"\"\"\n",
    "    idx_to_tok = OrderedDict()\n",
    "    tok_to_idx = OrderedDict()\n",
    "\n",
    "    curr_idx = 1\n",
    "    for tok in df['token']:\n",
    "        if tok not in tok_to_idx:\n",
    "            tok_to_idx[tok] = curr_idx\n",
    "            idx_to_tok[curr_idx] = tok\n",
    "            curr_idx += 1\n",
    "\n",
    "    tok_listing = list(idx_to_tok.values())\n",
    "\n",
    "    return idx_to_tok, tok_to_idx, tok_listing\n",
    "\n",
    "\n",
    "idx_to_tok, tok_to_idx, tok_listing = build_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fshhmnqnx_Mt"
   },
   "source": [
    "Once the vocabulary is built, we perform some sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(idx_to_tok) == len(tok_to_idx)\n",
    "assert len(idx_to_tok) == len(tok_listing)\n",
    "\n",
    "for i in range(1, len(idx_to_tok) + 1):\n",
    "    assert idx_to_tok[i] in tok_to_idx\n",
    "    assert tok_to_idx[idx_to_tok[i]] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMdjicUeyGcn"
   },
   "source": [
    "And then save the vocabulary for a more detailed inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(os.getcwd(), 'vocab.json')\n",
    "\n",
    "with open(vocab_path, 'w') as f:\n",
    "    sj.dump(idx_to_tok, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document visualization\n",
    "To gain more insight on the dataset, and on the classified words next, we can visualize the dataset in a human readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping between POS tags, their meaning and some colors\n",
    "tag_map = {\n",
    "    'CC': ('Coordin. Conjunction', '#c18401'),\n",
    "    'TO': ('“to”', '#c18401'),\n",
    "    'DT': ('Determiner', '#c18401'),\n",
    "    'UH': ('Interjection', '#c18401'),\n",
    "    'EX': ('Existential ‘there', '#c18401'),\n",
    "    'MD': ('Modal can', '#c18401'),\n",
    "    'LS': ('List item marker', '#c18401'),\n",
    "    'IN': ('Preposition/sub-conj', '#c18401'),\n",
    "    'CD': ('Cardinal number', '#282828'),\n",
    "    'FW': ('Foreign word', '#282828'),\n",
    "    'NN': ('Noun, singular/mass', '#282828'),\n",
    "    'NNS': ('Noun, plural', '#282828'),\n",
    "    'NNP': ('Proper noun, singul.', '#282828'),\n",
    "    'NNPS': ('Proper noun, plural', '#282828'),\n",
    "    'JJ': ('Adjective', '#50a14f'),\n",
    "    'JJR': ('Adj. comparative ', '#50a14f'),\n",
    "    'JJS': ('Adj. superlative ', '#50a14f'),\n",
    "    'VB': ('Verb, base form', '#e45649'),\n",
    "    'VBD': ('Verb, past tense ', '#e45649'),\n",
    "    'VBG': ('Verb, gerund ', '#e45649'),\n",
    "    'VBN': ('Verb, past particip. ', '#e45649'),\n",
    "    'VBP': ('Verb, non-3sg pres', '#e45649'),\n",
    "    'VBZ': ('Verb, 3sg pres ', '#e45649'),\n",
    "    'WDT': ('Wh-determiner', '#4078f2'),\n",
    "    'WP': ('Wh-pronoun', '#4078f2'),\n",
    "    'WP$': (' Possessive wh-', '#4078f2'),\n",
    "    'WRB': ('Wh-adverb how', '#4078f2'),\n",
    "    'PDT': ('Predeterminer ', '#4078f2'),\n",
    "    'POS': ('Possessive ending', '#4078f2'),\n",
    "    'PP': ('Personal pronoun', '#4078f2'),\n",
    "    'PP$': (' Possessive pronoun ', '#4078f2'),\n",
    "    'RB': ('Adverb', '#a626a4'),\n",
    "    'RBR': ('Adverb, comparative', '#a626a4'),\n",
    "    'RBS': ('Adverb, superlative', '#a626a4'),\n",
    "    'RP': ('Particle', '#a626a4'),\n",
    "}\n",
    "\n",
    "\n",
    "def display_pos_tagging(tokens: pd.Series,\n",
    "                        predicted_tags: pd.Series,\n",
    "                        correct_tags: pd.Series = None,\n",
    "                        limit=1000):\n",
    "    # If no correct tags are passed, we ignore the \"error highlighting\"\n",
    "    if correct_tags is None:\n",
    "        correct_tags = predicted_tags\n",
    "\n",
    "    # Limit the inputs\n",
    "    tokens = tokens[:limit]\n",
    "    predicted_tags = predicted_tags[:limit]\n",
    "    correct_tags = correct_tags[:limit]\n",
    "\n",
    "    # Iterate through tokens and tags, generating styled html based on the color\n",
    "    html_sequence = []\n",
    "    for token, tag, correct in zip(tokens, predicted_tags, correct_tags):\n",
    "        tag_meaning = tag_map.get(tag, ('', ''))[0]\n",
    "        err = 'error' if tag != correct else ''\n",
    "        h = f'<div class=\"token {tag} {err}\">{token} <span class=\"tag\">[{tag}] {tag_meaning}</span></div>'\n",
    "        if tag == '.':\n",
    "            h += '<div class=\"separator\"/>'\n",
    "        html_sequence.append(h)\n",
    "    html_body = '<div class=\"pos-visualizer\">'\n",
    "    html_body += ' '.join(html_sequence) + '</div>'\n",
    "\n",
    "    # Generate the style (WARNING: CSS lies ahead)\n",
    "    html_style = \"\"\"\n",
    "    <style>\n",
    "    .pos-visualizer { margin: 32px;}\n",
    "    .token { position:relative; display:inline-block; font-size:16px;}\n",
    "    .token .tag { \n",
    "        visibility:hidden; width: 120px; text-align:center; position:absolute;\n",
    "        width: 160px; background-color: #282828; color: #fff; border-radius: 6px;\n",
    "        z-index: 1; bottom: 100%; left: 50%; margin-left:-80px; font-size:12px;\n",
    "    }\n",
    "    .error { text-decoration: underline solid #F94144;}\n",
    "    .separator { margin-top:12px }\n",
    "    .token:hover .tag { visibility:visible }\n",
    "    \"\"\"\n",
    "    html_style += '\\n'.join(\n",
    "        (f'.{tag} {{color:{tag_map.get(tag, (\"\", \"#282828\"))[1]};}}'\n",
    "         for tag in predicted_tags.unique()))\n",
    "    html_style += '</style>'\n",
    "\n",
    "    # Display the html in the cell's output\n",
    "    display(HTML(html_style + html_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample POS tagging (hover on text for tag meaning)\n",
    "predicted_example = dataset['tag'].copy()\n",
    "predicted_example[0:8] = 'CD'  # Wrong prediction example\n",
    "display_pos_tagging(dataset['token'],\n",
    "                    predicted_example,\n",
    "                    dataset['tag'],\n",
    "                    limit=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the sentences containing the \"Wa\" token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_sentences = dataset[dataset['token'].eq('Wa')]['sentence']\n",
    "wa_df = dataset[dataset['sentence'].isin(wa_sentences)]\n",
    "display_pos_tagging(wa_df['token'], wa_df['tag'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
