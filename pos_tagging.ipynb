{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3B9biLeVvc1"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Model definition\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, TimeDistributed, Dense\n",
    "\n",
    "# Data packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import glob\n",
    "\n",
    "# Cloning\n",
    "from copy import deepcopy\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pathlib\n",
    "\n",
    "# Notebook visualization\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Typing\n",
    "from typing import Set\n",
    "\n",
    "# For GloVe wrapper\n",
    "!pip install gensim -U\n",
    "import gensim\n",
    "from gensim import downloader as gensloader\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## 1 - Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### 1.1 - Data loading\n",
    "First, we load the dataset and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document  sentence   token  tag\n",
       "0         0         0  Pierre  NNP\n",
       "1         0         0  Vinken  NNP\n",
       "2         0         0       ,    ,\n",
       "3         0         0      61   CD\n",
       "4         0         0   years  NNS"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = './dependency_treebank'  # Change if dataset already present locally\n",
    "DATASET_URL = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not pathlib.Path(ds_path).exists():\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']:\n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI9ECEh3Vvc6"
   },
   "source": [
    "### 1.2 - GloVe loading\n",
    "Then, we load the GloVe embeddings (GloVe-50, to be precise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 50\n",
    "GLOVE_TYPE = f'glove-wiki-gigaword-{EMB_DIM}'\n",
    "GLOVE_FILE = f'./glove/glove-wiki-gigaword-{EMB_DIM}.kv'\n",
    "\n",
    "\n",
    "def load_glove(gl_file: str, gl_type: str) -> KeyedVectors:\n",
    "    # Load local version\n",
    "    path = pathlib.Path(gl_file)\n",
    "    if path.exists():\n",
    "        return gensim.models.KeyedVectors.load(gl_file)\n",
    "\n",
    "    # Otherwise download and store glove\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    glove = gensloader.load(gl_type)\n",
    "    glove.save(gl_file)\n",
    "    return glove\n",
    "\n",
    "\n",
    "glove = load_glove(GLOVE_FILE, GLOVE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat = [ 0.45281  -0.50108  -0.53714  -0.015697  0.22191   0.54602  -0.67301\n",
      " -0.6891    0.63493  -0.19726   0.33685   0.7735    0.90094   0.38488\n",
      "  0.38367   0.2657   -0.08057   0.61089  -1.2894   -0.22313  -0.61578\n",
      "  0.21697   0.35614   0.44499   0.60885  -1.1633   -1.1579    0.36118\n",
      "  0.10466  -0.78325   1.4352    0.18629  -0.26112   0.83275  -0.23123\n",
      "  0.32481   0.14485  -0.44552   0.33497  -0.95946  -0.097479  0.48138\n",
      " -0.43352   0.69455   0.91043  -0.28173   0.41637  -1.2609    0.71278\n",
      "  0.23782 ]\n"
     ]
    }
   ],
   "source": [
    "# Test GloVe loading\n",
    "print(f'cat = {glove[\"cat\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlL7FRunWqwE"
   },
   "source": [
    "### 1.3 - Data visualization\n",
    "One of the most important ML tasks is getting familiar with the data in order to gain a deeper insight on their structure and nature.\n",
    "\n",
    "To do so, we define a function that displays tokens with their POS tags in a human-friendlier way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this could be put in its own file to keep things clean,\n",
    "#       but to upload just one notebook we instead code-golfed a bit :)\n",
    "\n",
    "# Define a mapping between POS tags, their meaning and some colors\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_map = {\n",
    "    'CC': ('Coordin. Conjunction', '#c18401'),\n",
    "    'TO': ('“to”', '#c18401'),\n",
    "    'DT': ('Determiner', '#c18401'),\n",
    "    'UH': ('Interjection', '#c18401'),\n",
    "    'EX': ('Existential ‘there', '#c18401'),\n",
    "    'MD': ('Modal can', '#c18401'),\n",
    "    'LS': ('List item marker', '#c18401'),\n",
    "    'IN': ('Preposition/sub-conj', '#c18401'),\n",
    "    'CD': ('Cardinal number', '#282828'),\n",
    "    'FW': ('Foreign word', '#282828'),\n",
    "    'NN': ('Noun, singular/mass', '#282828'),\n",
    "    'NNS': ('Noun, plural', '#282828'),\n",
    "    'NNP': ('Proper noun, singul.', '#282828'),\n",
    "    'NNPS': ('Proper noun, plural', '#282828'),\n",
    "    'JJ': ('Adjective', '#50a14f'),\n",
    "    'JJR': ('Adj. comparative ', '#50a14f'),\n",
    "    'JJS': ('Adj. superlative ', '#50a14f'),\n",
    "    'VB': ('Verb, base form', '#e45649'),\n",
    "    'VBD': ('Verb, past tense ', '#e45649'),\n",
    "    'VBG': ('Verb, gerund ', '#e45649'),\n",
    "    'VBN': ('Verb, past particip. ', '#e45649'),\n",
    "    'VBP': ('Verb, non-3sg pres', '#e45649'),\n",
    "    'VBZ': ('Verb, 3sg pres ', '#e45649'),\n",
    "    'WDT': ('Wh-determiner', '#4078f2'),\n",
    "    'WP': ('Wh-pronoun', '#4078f2'),\n",
    "    'WP$': (' Possessive wh-', '#4078f2'),\n",
    "    'WRB': ('Wh-adverb how', '#4078f2'),\n",
    "    'PDT': ('Predeterminer ', '#4078f2'),\n",
    "    'POS': ('Possessive ending', '#4078f2'),\n",
    "    'PP': ('Personal pronoun', '#4078f2'),\n",
    "    'PP$': (' Possessive pronoun ', '#4078f2'),\n",
    "    'RB': ('Adverb', '#a626a4'),\n",
    "    'RBR': ('Adverb, comparative', '#a626a4'),\n",
    "    'RBS': ('Adverb, superlative', '#a626a4'),\n",
    "    'RP': ('Particle', '#a626a4'),\n",
    "}\n",
    "tag_map = defaultdict(lambda: ('', '#282828'), tag_map)\n",
    "\n",
    "\n",
    "def display_pos_tagging(tokens: pd.Series,\n",
    "                        predicted_tags: pd.Series,\n",
    "                        correct_tags: pd.Series = None,\n",
    "                        limit=1000):\n",
    "    # If no correct tags are passed, we ignore the \"error highlighting\"\n",
    "    if correct_tags is None:\n",
    "        correct_tags = predicted_tags\n",
    "\n",
    "    # Limit the inputs\n",
    "    tokens = tokens[:limit]\n",
    "    predicted_tags = predicted_tags[:limit]\n",
    "    correct_tags = correct_tags[:limit]\n",
    "\n",
    "    # Iterate through tokens and tags, generating styled HTML based on the tags\n",
    "    html_sequence = []\n",
    "    for token, tag, correct in zip(tokens, predicted_tags, correct_tags):\n",
    "        tag_meaning = tag_map[tag][0]\n",
    "        err = 'pos-error' if tag != correct else ''\n",
    "        h = f'<div class=\"token {tag} {err}\">{token} <span class=\"tag\">[{tag}] {tag_meaning}</span></div>'\n",
    "        if tag == '.':\n",
    "            h += '<div class=\"separator\"/>'\n",
    "        html_sequence.append(h)\n",
    "    html_body = '<div class=\"pos-visualizer\">'\n",
    "    html_body += ' '.join(html_sequence) + '</div>'\n",
    "\n",
    "    # Generate the style (WARNING: CSS lies ahead)\n",
    "    html_style = \"\"\"\n",
    "\t<style>\n",
    "\t.pos-visualizer { padding: 32px; background-color: #FEFEFE; border-left:solid 1px grey;}\n",
    "\t.token { position:relative; display:inline-block; font-size:16px;}\n",
    "\t.token .tag { \n",
    "\t\tvisibility:hidden; width: 120px; text-align:center; position:absolute;\n",
    "\t\twidth: 160px; background-color: #282828; color: #fff; border-radius: 6px;\n",
    "\t\tz-index: 1; bottom: 100%; left: 50%; margin-left:-80px; font-size:12px;\n",
    "\t}\n",
    "\t.pos-error { text-decoration: underline solid #F94144;}\n",
    "\t.separator { margin-top:12px }\n",
    "\t.token:hover .tag { visibility:visible }\n",
    "\t\"\"\"\n",
    "    html_style += '\\n'.join((f'.{tag} {{color:{tag_map[tag][1]};}}'\n",
    "                             for tag in predicted_tags.unique()))\n",
    "    html_style += '</style>'\n",
    "\n",
    "    # Display the HTML in the cell's output\n",
    "    display(HTML(html_style + html_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample POS tagging (hover on text for tag meaning)\n",
    "predicted_example = dataset['tag'].copy()\n",
    "predicted_example[0:8] = 'CD'  # Wrong prediction example for the first 8 words\n",
    "display_pos_tagging(dataset['token'],\n",
    "                    predicted_example,\n",
    "                    dataset['tag'],\n",
    "                    limit=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### 1.4 - Pre-processing\n",
    "Our dataset is already relatively clean; however, one point that might be worth considering is how to handle lowercase conversions. Some tokens in our dataset will be intrinsically capitalized (e.g. proper nouns, the personal pronoun \"I\"), whereas some other will be capitalized only because they follow a period in the sentence they occur in.\n",
    "\n",
    "One might think of converting a token to lowercase based on its tag (e.g. if a token is a proper noun, keep it capitalized); however, to be fair, this could only be done on the training set, since in a real scenario test-set tags would be unknown.\n",
    "\n",
    "Anyway, all these considerations hold only if GloVe contains embeddings of capitalized words; if that's not the case, every word we keep as capitalized will be classified as OOV when matched with GloVe, even when their lowercase embedding actually exists.\n",
    "\n",
    "As it turns out, Glove does not encode capitalized words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_capitalized = len(list(filter(lambda w: w[0].isupper(), glove.key_to_index.keys())))\n",
    "\n",
    "print(f'GloVe-50 encodes {num_capitalized} capitalized words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kqy2eiLVvdC"
   },
   "source": [
    "Therefore, we will be forced to convert all tokens to lowercase.\n",
    "\n",
    "We are also interested to see which \"special\" tokens are encoded in GloVe, i.e. punctuation, quotation marks, and tokens such as \"-LRB-\" and \"-RRB-\", which in our dataset replace \"(\" and \")\", respectively.\n",
    "\n",
    "As it turns out, GloVe contains every special symbol we care about, except for tokens reserved to brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    *',.:;\"`$#£!%/?^-()[]{}_', \"''\", \"``\", \"--\", \"-LRB-\", \"-RRB-\", \"-LSB-\",\n",
    "    \"-RSB-\", \"-LCB-\", \"-RCB-\"\n",
    "]\n",
    "for st in special_tokens:\n",
    "    if st not in glove:\n",
    "        print(f\"GloVe does not contain token {st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2B391arVvdD"
   },
   "source": [
    "Based on the previous considerations, we convert all tokens to lowercase and replace \"-LRB\"-like symbols with the corresponding bracket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the brackets\n",
    "for token, bracket in [('-LRB-', '('), ('-RRB-', ')'), ('-LSB-', '['),\n",
    "                       ('-RSB-', ']'), ('-LCB-', '{'), ('-RCB-', '}')]:\n",
    "    dataset.loc[dataset.token == token, 'token'] = bracket\n",
    "\n",
    "# Convert dataset tokens to lowercase\n",
    "dataset.loc[:, 'token'] = dataset['token'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLuhT_wCVvdE"
   },
   "source": [
    "### 1.5 - Splitting\n",
    "After pre-processing the data, we can finally split the dataset into train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: \n",
      "    TRAIN: 100 documents, 47356 tokens\n",
      "    VALIDATION: 50 documents, 31183 tokens\n",
      "    TEST: 49 documents, 15545 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_train = dataset[dataset['document'].lt(100)]\n",
    "ds_val = dataset[dataset['document'].between(100, 149)].reset_index()\n",
    "ds_test = dataset[dataset['document'].gt(149)].reset_index()\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} tokens\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(ds_train)}\n",
    "    VALIDATION: {print_split(ds_val)}\n",
    "    TEST: {print_split(ds_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70PQNGtHVvdE"
   },
   "source": [
    "### 1.6 - OOV Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dii_iOaHI4t1"
   },
   "source": [
    "#### 1.6.1 - OOV Analysis\n",
    "First of all, let us take a look at how many Out-Of-Vocabulary tokens (w.r.t. GloVe) our dataset contains. In order to simulate a real-world scenario, in which test samples are not readily available at training time, we are going to check (and then handle) OOVs *incrementally*; that is, we will consider:\n",
    "* **Training OOVs:** training-set tokens which are not found in GloVe.\n",
    "* **Validation OOVs:** validation-set tokens which are not found in `union(`GloVe, Training OOVs`)`.\n",
    "* **Test OOVs:** test-set tokens which are not found in `union(`GloVe, Training OOVs, Validation OOVs`)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV tokens in dataset, considering INCREMENTAL OOV EMBEDDINGS:\n",
      "    TRAIN: 355 [4.79%]\n",
      "    VALIDATION: 189 [3.49%]\n",
      "    TEST: 128 [3.76%]\n",
      "    === TOTAL: 672 [6.14%]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_oov(tokens, embedding_keys):\n",
    "    return set(tokens) - set(embedding_keys)\n",
    "\n",
    "# NOTE _ TODO: gensim3 usa glove.vocab al posto di .key_to_index.keys()\n",
    "glove_keys = glove.key_to_index.keys()\n",
    "oov_train = get_oov(ds_train['token'].unique(), glove_keys)\n",
    "oov_val = get_oov(ds_val['token'].unique(), glove_keys)\n",
    "oov_test = get_oov(ds_test['token'].unique(), glove_keys)\n",
    "\n",
    "print_oov = lambda s, d: f\"{len(s)} [{len(s) / len(d['token'].unique()) * 100:.2f}%]\"\n",
    "print(f\"\"\"Number of OOV tokens in dataset, considering INCREMENTAL OOV EMBEDDINGS:\n",
    "    TRAIN: {print_oov(oov_train, ds_train)}\n",
    "    VALIDATION: {print_oov(oov_val - oov_train, ds_val)}\n",
    "    TEST: {print_oov(oov_test - (oov_val | oov_train), ds_test)}\n",
    "    === TOTAL: {print_oov(oov_train | oov_val | oov_test, dataset)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJkqb0b9VvdH"
   },
   "source": [
    "#### 1.6.2 - Adding OOVs to GloVe\n",
    "We can now add OOV tokens to the GloVe vocabulary. Many strategies can be adopted to encode OOVs as vectors:\n",
    "1. Static embeddings with the same vector for all OOV tokens (e.g. zeros).\n",
    "2. Random embeddings. \n",
    "3. Computing an embedding as some statistic involving neighboring tokens (e.g. their mean).\n",
    "\n",
    "Two observations can guide us in the choice of an embedding strategy:\n",
    "* OOV tokens are not negligible (about 6% of the *total* dataset).\n",
    "* Our GloVe embeddings will not undergo further training, therefore fixed or random embedding values will not be refined during the training process.\n",
    "\n",
    "For the two reasons above, given an OOV token, we will compute its embedding as the mean of its left and right neighbors across all its occurrences throughout the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighbor_mean(oov_token: str, df: pd.DataFrame,\n",
    "                          embeddings: KeyedVectors) -> np.ndarray:\n",
    "    # Find indexes where the oov token appears, and shift them by -1 +1\n",
    "    indexes = df.index[df['token'] == oov_token].values\n",
    "    indexes = np.concatenate((indexes - 1, indexes + 1))\n",
    "\n",
    "    # For each oov word index, look at the left and right until a word with embedding has been found\n",
    "    neighbor_embeddings = []\n",
    "    for idx in indexes:\n",
    "        for direction in (range(idx - 1, -1, -1), range(idx + 1, len(df))):\n",
    "            for i in direction:\n",
    "                tok = df['token'].iloc[i]\n",
    "                if tok not in embeddings:\n",
    "                    continue\n",
    "                vector = embeddings[tok]\n",
    "                neighbor_embeddings.append(vector)\n",
    "                break\n",
    "\n",
    "    return np.mean(neighbor_embeddings, axis=0)\n",
    "\n",
    "\n",
    "def add_oovs(oov_tokens: Set, df: pd.DataFrame, \n",
    "             embeddings: KeyedVectors) -> KeyedVectors:\n",
    "    # Clone the embedding (KeyedVectors does not have a clone method)\n",
    "    emb_filled = deepcopy(embeddings)\n",
    "\n",
    "    # Estimate the OOV embeddings\n",
    "    keys, values = [], []\n",
    "    for oov in oov_tokens:\n",
    "        vector = compute_neighbor_mean(oov, df, emb_filled)\n",
    "        keys.append(oov)\n",
    "        values.append(vector)\n",
    "    # Add the estimates to the embedding\n",
    "    emb_filled.add_vectors(keys, values)\n",
    "    return emb_filled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons as discussed in the previous section, we will build our vocabulary incrementally:\n",
    "1. Starting vocabulary V1 = GloVe.\n",
    "2. V2 = `union(`GloVe, Training OOVs`)`.\n",
    "3. V3 = `union(`V2, Validation OOVs`)`.\n",
    "4. Final vocabulary V4 = `union(`V3, Test OOVs`)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental addition to OOVs to GloVe embeddings\n",
    "embeddings = add_oovs(oov_train, ds_train, glove)  # add Training OOVs\n",
    "embeddings = add_oovs(oov_val - oov_train, ds_val, embeddings)  # add Validation OOVs\n",
    "embeddings = add_oovs(oov_test - oov_val - oov_train, ds_test, embeddings)  # add Test OOVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in original GloVe:                  400000\n",
      "Number of vectors after incremental addition of OOVs: 400672\n"
     ]
    }
   ],
   "source": [
    "# Test number of embeddings\n",
    "print(f'Number of vectors in original GloVe:                  {len(glove)}')\n",
    "print(f'Number of vectors after incremental addition of OOVs: {len(embeddings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIasDlmeJqMa"
   },
   "source": [
    "### 1.7 - Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indexes used for word -> index -> embedding\n",
    "word_index = {k: v+1 for k, v in embeddings.key_to_index.items()}  # +1 because index 0...\n",
    "vocab_size = len(embeddings) + 1                                   # ...will be reserved to padding\n",
    "\n",
    "# Define the embedding matrix\n",
    "embedding_matrix = np.zeros(shape=(vocab_size, EMB_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_matrix[index] = embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "assert np.all(embedding_matrix[embeddings.key_to_index['cat']+1] == embeddings['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCdBJQmXHLNZ"
   },
   "source": [
    "### 1.8 - Data Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkaqUqR2Bfza"
   },
   "source": [
    "The input data of our model could be either whole documents or single sentences contained in those documents; we will choose sentences as input data.\n",
    "\n",
    "Tokens in each sentence will be converted to integer sequences and later fed into a static `Embedding` layer storing the matrix of Glove encodings + OOVs, which will provide the input to our model.\n",
    "\n",
    "The corresponding tags—i.e. the output of our model—will be instead one-hot encoded. The rationale behind this choice is that tags are purely categorical data, hence encoding them as integer sequences would inject a notion of ordering into the model, which however is not reflected in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "      <th>token_index</th>\n",
       "      <th>tag_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[5030, 400137, 2, 4979, 83, 168, 2, 44, 1430, ...</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 8, 5, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "      <td>[1996, 400137, 15, 664, 4, 43651, 60024, 2, 1,...</td>\n",
       "      <td>[1, 1, 12, 9, 10, 1, 1, 2, 8, 1, 13, 9, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[9951, 34239, 2, 3069, 83, 168, 6, 158, 664, 4...</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5, 14, 5, 9, 10, 1, 1, 1, 1, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>3</th>\n",
       "      <td>[a, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[8, 684, 4, 14666, 443, 181, 5, 160, 5616, 701...</td>\n",
       "      <td>[8, 9, 10, 9, 17, 16, 18, 7, 1, 9, 4, 12, 16, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "      <td>[1, 14666, 7677, 2, 400286, 2, 15, 7683, 18184...</td>\n",
       "      <td>[8, 9, 9, 2, 9, 2, 12, 17, 5, 10, 19, 12, 8, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               token  \\\n",
       "document sentence                                                      \n",
       "0        0         [pierre, vinken, ,, 61, years, old, ,, will, j...   \n",
       "         1         [mr., vinken, is, chairman, of, elsevier, n.v....   \n",
       "1        2         [rudolph, agnew, ,, 55, years, old, and, forme...   \n",
       "2        3         [a, form, of, asbestos, once, used, to, make, ...   \n",
       "         4         [the, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                                                 tag  \\\n",
       "document sentence                                                      \n",
       "0        0         [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "         1         [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...   \n",
       "1        2         [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "2        3         [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "         4         [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...   \n",
       "\n",
       "                                                         token_index  \\\n",
       "document sentence                                                      \n",
       "0        0         [5030, 400137, 2, 4979, 83, 168, 2, 44, 1430, ...   \n",
       "         1         [1996, 400137, 15, 664, 4, 43651, 60024, 2, 1,...   \n",
       "1        2         [9951, 34239, 2, 3069, 83, 168, 6, 158, 664, 4...   \n",
       "2        3         [8, 684, 4, 14666, 443, 181, 5, 160, 5616, 701...   \n",
       "         4         [1, 14666, 7677, 2, 400286, 2, 15, 7683, 18184...   \n",
       "\n",
       "                                                           tag_index  \n",
       "document sentence                                                     \n",
       "0        0         [1, 1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 8, 5, 9,...  \n",
       "         1               [1, 1, 12, 9, 10, 1, 1, 2, 8, 1, 13, 9, 11]  \n",
       "1        2         [1, 1, 2, 3, 4, 5, 14, 5, 9, 10, 1, 1, 1, 1, 2...  \n",
       "2        3         [8, 9, 10, 9, 17, 16, 18, 7, 1, 9, 4, 12, 16, ...  \n",
       "         4         [8, 9, 9, 2, 9, 2, 12, 17, 5, 10, 19, 12, 8, 4...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility function\n",
    "flatten_1d = lambda nested_list: [li[0] for li in nested_list]\n",
    "\n",
    "# Convert tokens into sequences (their vocabulary indexes)\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.word_index = word_index\n",
    "token_indexes = tokenizer.texts_to_sequences_generator(dataset['token'].array)\n",
    "token_indexes = flatten_1d(token_indexes)\n",
    "\n",
    "# Convert tags into sequences\n",
    "# (as an intermediate step before one-hot encoding them)\n",
    "tag_int_map = {k: v+1 for v, k in enumerate(dataset['tag'].unique())}\n",
    "num_tags = len(tag_int_map) + 1\n",
    "tokenizer = Tokenizer(filters='', lower=False)\n",
    "tokenizer.word_index = tag_int_map\n",
    "tag_indexes = tokenizer.texts_to_sequences_generator(dataset['tag'].array)\n",
    "tag_indexes = flatten_1d(tag_indexes)\n",
    "\n",
    "# Augment dataset with new data\n",
    "dataset['token_index'] = token_indexes\n",
    "dataset['tag_index'] = tag_indexes\n",
    "\n",
    "# Group dataset by 'sentence', aggregating remaining data into lists\n",
    "ds_sentences = dataset.groupby(['document', 'sentence']).agg(list)\n",
    "ds_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of sentence length in the training + validation set (leaving the test set aside) to determine what is an appropriate padded-sequence size (for batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99th percentile of sentence length in training + validation set: 56.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences_len = ds_sentences.query('document < 149')['tag'].transform(len)\n",
    "len_quantile = sentences_len.quantile(.99)\n",
    "print(\"99th percentile of sentence length in training + validation set:\", len_quantile)\n",
    "\n",
    "sns.histplot(sentences_len)\n",
    "plt.axvline(len_quantile, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 99th percentile suggests to trim sentences that exceed 56 tokens and pad sentences with fewer tokens, in order to prevent the few outliers from causing sentence encodings to be wastefully long.  We choose pre-padding, as it has been experimentally proven to be more effective than its counterpart post-padding.\n",
    "> TODO: visto che vogliamo fare così i raffinati, a sto punto citiamo il paper come si deve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X shapes [sentences x tokens]\n",
      "    x_train.shape = (1963, 56)\n",
      "    x_val.shape   = (1290, 56)\n",
      "    x_test.shape  = (661, 56)\n",
      "\n",
      "Y shapes [sentences x tags x one-hot-size]\n",
      "    y_train.shape = (1963, 56, 46)\n",
      "    y_val.shape   = (1290, 56, 46)\n",
      "    y_test.shape  = (661, 56, 46)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pad = lambda x: pad_sequences(x, maxlen=int(len_quantile), padding='pre', truncating='pre')\n",
    "\n",
    "def get_model_data(df: pd.DataFrame, lb=0, hb=None):\n",
    "    hb = hb if hb is not None else len(df)\n",
    "    df = df.query(f'{lb} <= document < {hb}')\n",
    "    toks = pad(df['token_index'])\n",
    "    tags = pad(df['tag_index'])\n",
    "    # One-hot encode tags\n",
    "    tags = to_categorical(tags, num_classes=num_tags)\n",
    "    return toks, tags\n",
    "\n",
    "# Build the data that will be fed to the model\n",
    "x_train, y_train = get_model_data(ds_sentences, 0, 100)\n",
    "x_val, y_val = get_model_data(ds_sentences, 100, 149)\n",
    "x_test, y_test = get_model_data(ds_sentences, 149)\n",
    "\n",
    "# Check shapes\n",
    "print(f\"\"\"\n",
    "X shapes [sentences x tokens]\n",
    "    x_train.shape = {x_train.shape}\n",
    "    x_val.shape   = {x_val.shape}\n",
    "    x_test.shape  = {x_test.shape}\n",
    "\n",
    "Y shapes [sentences x tags x one-hot-size]\n",
    "    y_train.shape = {y_train.shape}\n",
    "    y_val.shape   = {y_val.shape}\n",
    "    y_test.shape  = {y_test.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CyRBTMKLT1i"
   },
   "source": [
    "## 2 - Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 23:33:57.513705: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 56, 50)            20033650  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 56, 64)           21248     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 56, 46)           2990      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,057,888\n",
      "Trainable params: 24,238\n",
      "Non-trainable params: 20,033,650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    EMB_DIM, \n",
    "                    input_length=x_train.shape[-1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(num_tags, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKmBM2ZEQsOc"
   },
   "source": [
    "## 3 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(x_train, y_train,\n",
    "#                     epochs=10,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50ccc516328d5b564b2da3ee1dd645a78b56fd0f0e8585996496188dec9b39eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
