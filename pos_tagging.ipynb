{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWqbqbGHGA_K"
   },
   "source": [
    "# Assignment 1: Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQQcUC98GA_O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System packages\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# File management\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Types and type-annotations\n",
    "from typing import List, Dict\n",
    "from collections import OrderedDict\n",
    "\n",
    "# To store vocabulary as .json\n",
    "!pip install simplejson\n",
    "import simplejson as sj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwHwCUJEFpJ"
   },
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO1uB0GnGA_R"
   },
   "source": [
    "### Data Loading\n",
    "First we load the data (downloading it if not present), and store it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu5i9VRDGA_S"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./dependency_treebank\"\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "\n",
    "def load_dataset(ds_path: str, ds_url: str) -> pd.DataFrame:\n",
    "    # Check if dataset is already present, otherwise download it\n",
    "    if not os.path.isdir(ds_path):\n",
    "        request_zip = requests.get(ds_url, stream=True)\n",
    "        zip = zipfile.ZipFile(io.BytesIO(request_zip.content))\n",
    "        zip.extractall()\n",
    "\n",
    "    # Load each file into a list\n",
    "    documents = []\n",
    "    for file_name in sorted(glob.glob(f\"{ds_path}/*.dp\")):\n",
    "        with open(file_name) as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    # Convert each row of the documents into a list\n",
    "    raw_df = []\n",
    "    sentence_idx = 0\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        rows = doc.split('\\n')\n",
    "        for row in rows:\n",
    "            cols = row.split('\\t')[:2]  # Ignore the last column\n",
    "            if cols == ['']: \n",
    "                sentence_idx += 1\n",
    "            else:\n",
    "                raw_df.append([doc_idx, sentence_idx, *cols])\n",
    "\n",
    "    # Finally, convert the nested list into a pandas dataframe\n",
    "    df = pd.DataFrame(raw_df, columns=['document', 'sentence', 'token', 'tag'])\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset(DATASET_PATH, DATASET_URL)\n",
    "dataset[dataset['document'].lt(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwW2BJ_Eted"
   },
   "source": [
    "### Data Pre-Processing\n",
    "The dataset does not need much of a cleanup: the only pre-processing we need to perform is converting tokens to lowercase, so that we can create a vocabulary without ending up with two entries for the same word.\n",
    "\n",
    "However, we need to distinguish between words that are inherently capitalized (e.g. proper nouns) and those that are uppercase just because they follow a period.\n",
    "\n",
    "First of all, let us check which kinds of tags produce capitalized words, and how many those words are for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1637275550519,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "VlFHftXOFjHI",
    "outputId": "80f842ff-4747-4151-98fa-62de15e8d4a9"
   },
   "outputs": [],
   "source": [
    "dataset[dataset['token'].str.isupper()].groupby('tag').size().reset_index(name='uppercase counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCnFAL4dOl9N"
   },
   "source": [
    "The meaning of the tags which have occurrences of capitalized words are (see [here](https://sites.google.com/site/partofspeechhelp/)):\n",
    "\n",
    "* $: Dollar mark\n",
    "* LRB: Left hand side brackets\n",
    "* RRB: Right hand side brackets\n",
    "* CD: Cardinal Numbers\n",
    "* DT: Determiners\n",
    "* IN: Prepositions and Subordinating Conjunctions\n",
    "* JJ: Adjectives\n",
    "* NN: Common Nouns (Singular or Mass)\n",
    "* **NNP: Proper Nouns (Singular)**\n",
    "* **NNPS: Proper Nouns (Plural)**\n",
    "* NNS: Common Nouns (Plural)\n",
    "* **PRP: Personal Pronouns**\n",
    "* RB: Adverbs\n",
    "* TO: to\n",
    "* UH: Interjection\n",
    "* VBD: Verbs (past tense)\n",
    "* VBG: Verbs (gerund or present participle)\n",
    "* VBN: Verbs (past participle)\n",
    "* VBZ: Verbs (3rd person singular present)\n",
    "* WDT: Wh-determiner\n",
    "\n",
    "> TODO: Why do parentheses and the dollar sign come up as uppercase?\n",
    "\n",
    "[TBC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B2Vsj-IGA_S"
   },
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1637274720898,
     "user": {
      "displayName": "Francesco Ballerini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg-R7R-HjBq_dn0hwuUoDXC2RwAVd1JXz6dReMZ3g=s64",
      "userId": "00684324433983888967"
     },
     "user_tz": -60
    },
    "id": "VjSpMtJzGA_T",
    "outputId": "e624f533-5eb0-49a7-ef0d-d5c9eb428b75"
   },
   "outputs": [],
   "source": [
    "train_ds = dataset[dataset['document'].lt(100)]\n",
    "validation_ds = dataset[dataset['document'].between(100, 149)]\n",
    "test_ds = dataset[dataset['document'].gt(149)]\n",
    "\n",
    "print_split = lambda df: f\"{df.groupby('document').ngroups} documents, {len(df)} samples\"\n",
    "print(f\"\"\"Dataset split: \n",
    "    TRAIN: {print_split(train_ds)}\n",
    "    VALIDATION: {print_split(validation_ds)}\n",
    "    TEST: {print_split(test_ds)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW8zQ7RuGA_U"
   },
   "source": [
    "### Vocaboulary Creation\n",
    "> TODO: lowercase conversion before vocabulaty creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLbXKgAmLeE3"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str],\n",
    "                                           Dict[str, int],\n",
    "                                           List[str]):\n",
    "    \"\"\"Given a dataset, builds the corresponding token vocabulary.\n",
    "    The vocabulary starts from index 1 so as to allow the 0 slot to be reserved to the padding token.\n",
    "\n",
    "    Args:\n",
    "        df: dataset, assumed to have a 'token' column.\n",
    "\n",
    "    Returns:\n",
    "        idx_to_pos: token vocabulary, i.e. from index to token.\n",
    "        pos_to_idx: inverse token vocabulary, i.e. from token to index.\n",
    "        pos_listing: list of unique tokens that build up the vocabulary.\n",
    "    \"\"\"\n",
    "    idx_to_tok = OrderedDict()\n",
    "    tok_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 1\n",
    "    for tok in df['token']:\n",
    "        if tok not in tok_to_idx:\n",
    "            tok_to_idx[tok] = curr_idx\n",
    "            idx_to_tok[curr_idx] = tok\n",
    "            curr_idx += 1\n",
    "\n",
    "    tok_listing = list(idx_to_tok.values())\n",
    "\n",
    "    return idx_to_tok, tok_to_idx, tok_listing\n",
    "\n",
    "\n",
    "idx_to_tok, tok_to_idx, tok_listing = build_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fshhmnqnx_Mt"
   },
   "source": [
    "Once the vocabulary is built, we perform some sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK-x0Jf1mp6j"
   },
   "outputs": [],
   "source": [
    "assert len(idx_to_tok) == len(tok_to_idx)\n",
    "assert len(idx_to_tok) == len(tok_listing)\n",
    "\n",
    "for i in range(1, len(idx_to_tok) + 1):\n",
    "    assert idx_to_tok[i] in tok_to_idx\n",
    "    assert tok_to_idx[idx_to_tok[i]] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMdjicUeyGcn"
   },
   "source": [
    "And then save the vocabulary for a more detailed inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hs6V3gSZxjZb"
   },
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(os.getcwd(), 'vocab.json')\n",
    "\n",
    "with open(vocab_path, 'w') as f:\n",
    "    sj.dump(idx_to_tok, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pos_tagging.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "2cdbff7efa5df5bfb11dcd583c56ac0f576766c47f09cab205fa9f1668d16c1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
